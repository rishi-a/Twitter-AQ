{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CVBertTransfer-Sentimnet140.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sWEMht9UPR94"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t03PZ6l1Nada",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "71399425-2ea8-4efc-886c-4d0c0feb04c9"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download kazanova/sentiment140 -f training.1600000.processed.noemoticon.csv\n",
        "!unzip /content/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "training.1600000.processed.noemoticon.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  /content/training.1600000.processed.noemoticon.csv.zip\n",
            "replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hm2IkVyffbe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "70c40bd6-b695-417f-f571-816576c5735d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkziAIlUPjkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip -q install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eMVSfoohyM_x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cd238a8-faa4-4330-8ee4-69b510050ad7"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pN5OGfw30-YV"
      },
      "source": [
        "## Loading and preprocessing dataset (1,0,-1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QMlW9DRsI3gJ",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_colwidth', 2000) # To visualize full output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R4dQDAB0J-Gj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "b58cee7c-5966-4539-ddf6-5ff85042201e"
      },
      "source": [
        "url = '/content/training.1600000.processed.noemoticon.csv'\n",
        "df_raw = pd.read_csv(url,encoding='latin-1',header=None)\n",
        "#df_main = df_main.rename(columns = {'Zlabel':'ZLabel', 'Tlabel':'TLabel', 'text':'tweet'})\n",
        "df_raw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601966</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601969</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601991</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602064</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>tinydiamondz</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602129</td>\n",
              "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>RyanTrevMorris</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0  ...                                                                                                                    5\n",
              "0        0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
              "1        0  ...      is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
              "2        0  ...                            @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
              "3        0  ...                                                                      my whole body feels itchy and like its on fire \n",
              "4        0  ...      @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
              "...     ..  ...                                                                                                                  ...\n",
              "1599995  4  ...                                                             Just woke up. Having no school is the best feeling ever \n",
              "1599996  4  ...                                       TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta\n",
              "1599997  4  ...                                                            Are you ready for your MoJo Makeover? Ask me for details \n",
              "1599998  4  ...                                                    Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur \n",
              "1599999  4  ...                                                       happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H \n",
              "\n",
              "[1600000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQQTdRriQl1Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "43ce19a2-22c0-4ad4-f786-a70612f73fe7"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['tweet'] = df_raw[5]\n",
        "df['ZLabel'] = df_raw[0].replace({0:0,2:1,4:2})\n",
        "df['ZLabel'].value_counts()\n",
        "df = df.sample(n=50000, random_state=42)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>ZLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>541200</th>\n",
              "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750</th>\n",
              "      <td>@misstoriblack cool , i have no tweet apps  for my razr 2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766711</th>\n",
              "      <td>@TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like have a sleepover or whatever, ill call u</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285055</th>\n",
              "      <td>School email won't open  and I have geography stuff on there to revise! *Stupid School* :'(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>705995</th>\n",
              "      <td>upper airways problem</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199266</th>\n",
              "      <td>On the way home. Tired. I don't wanna babysit</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210814</th>\n",
              "      <td>Well that was ten sneezes in a row. My allergies are not making my day easy. Still no air through the nose.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180674</th>\n",
              "      <td>can't go out tonight car is acting funny   wat shall i do???</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364859</th>\n",
              "      <td>Headacheeeee</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172400</th>\n",
              "      <td>@tommcfly you and the guys should come down here, we are fucking freezing out here!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                            tweet  ZLabel\n",
              "541200                                                                                                     @chrishasboobs AHHH I HOPE YOUR OK!!!        0\n",
              "750                                                                                     @misstoriblack cool , i have no tweet apps  for my razr 2       0\n",
              "766711  @TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like have a sleepover or whatever, ill call u       0\n",
              "285055                                                School email won't open  and I have geography stuff on there to revise! *Stupid School* :'(       0\n",
              "705995                                                                                                                     upper airways problem        0\n",
              "...                                                                                                                                           ...     ...\n",
              "199266                                                                                             On the way home. Tired. I don't wanna babysit        0\n",
              "210814                               Well that was ten sneezes in a row. My allergies are not making my day easy. Still no air through the nose.        0\n",
              "180674                                                                               can't go out tonight car is acting funny   wat shall i do???       0\n",
              "364859                                                                                                                              Headacheeeee        0\n",
              "172400                                                       @tommcfly you and the guys should come down here, we are fucking freezing out here!        0\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_BAkwS3KC0v",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def preprocess(tweet):\n",
        "  tweet = tweet.replace('@ ','@').replace('# ','#')\n",
        "  tweet = re.sub('pic.twitter.com.*','',tweet)\n",
        "  tweet = re.sub('https*://[^\\s]+','',tweet)\n",
        "  tweet = re.sub('https*://.*','',tweet)\n",
        "  tweet = ' '.join(w for w in tweet.split() if w[0] not in ['@']).lower()\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0RYK-SlCKFzN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26dfe67e-0559-4349-a53b-a37932b922ec"
      },
      "source": [
        "df['Tweet text processed'] = df['tweet'].apply(preprocess)\n",
        "#print(df.head())\n",
        "print('before len',len(df))\n",
        "df.drop_duplicates(subset='Tweet text processed',inplace=True)\n",
        "print('after len',len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before len 50000\n",
            "after len 49366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b1Cv-7M3j8T3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ef760744-4889-4fa2-9f25-079b423417cc"
      },
      "source": [
        "#hindi_inds = []\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "X = df['Tweet text processed']\n",
        "y = df['ZLabel'].astype('int').replace({2:1})\n",
        "#X = X.values.tolist()+append1['Tweet text processed'].values.tolist()+append2['Tweet text processed'].values.tolist()\n",
        "#y = y.values.tolist()+append1['ZLabel'].values.tolist()+append2['ZLabel'].values.tolist()\n",
        "#X = pd.Series(X)\n",
        "#X = df['tweet']\n",
        "#y = pd.Series(y).astype('int')+1\n",
        "y.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    24748\n",
              "1    24618\n",
              "Name: ZLabel, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xnf8EHj8163F"
      },
      "source": [
        "## Sentiment preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CMHciHRN4Muc",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vEkca5bS2I2M"
      },
      "source": [
        "#### finding out max sent length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JZKiztqU2AIZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c92bda3-bf00-4d0b-9a65-92ebdaa26c5b"
      },
      "source": [
        "max_len = 0\n",
        "for sent in X:\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0_mbKtyU2ULJ"
      },
      "source": [
        "#### Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3XmxdnE2Pus",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "250f0ddd-813a-4396-9fbf-f0e122700b0f"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in X:\n",
        "    \n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(y)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', X.iloc[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  ahhh i hope your ok!!!\n",
            "Token IDs: tensor([  101,  6289, 23644,  1045,  3246,  2115,  7929,   999,   999,   999,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dOxmlR1U4ZLG"
      },
      "source": [
        "#### Prepare and run the model for CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzXPdt8VMcVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0729e08f-8427-4f7c-bd9e-e6b6029019e0"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "99Pb8aHJKg-s",
        "colab": {}
      },
      "source": [
        "stats = {'train_loss':[None]*5, 'val_loss':[None]*5,'val_accuracy':[None]*5,\n",
        "         'test_preds':[], 'test_true':[],'test_inds':[], 'test_accuracy':[None]*5}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j72t2DHT4JNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "331c9995-8d1d-4003-e8a4-539c9e828cd3"
      },
      "source": [
        "# Set the seed value all over the place to make this reproducible.\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from torch.utils.data import TensorDataset\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "##################################################################################\n",
        "\n",
        "kfOuter = StratifiedKFold(10, shuffle=True, random_state=0)\n",
        "outerind = 0\n",
        "dict_hyp = {}\n",
        "for train_val_ind, test_ind in kfOuter.split(X, y):\n",
        "  train_val_dataset = TensorDataset(input_ids[train_val_ind], \n",
        "                                    attention_masks[train_val_ind], \n",
        "                                    labels[train_val_ind])\n",
        "  test_dataset = TensorDataset(input_ids[test_ind], attention_masks[test_ind], labels[test_ind])\n",
        "  kfInner = StratifiedKFold(10, shuffle=True, random_state=0)\n",
        "  train_ind, val_ind = next(kfInner.split(train_val_ind, labels[train_val_ind]))\n",
        "  train_df_ind, val_df_ind = train_val_ind[train_ind], train_val_ind[val_ind]\n",
        "  #smt = SMOTE(random_state=0)\n",
        "  #inp_over,label_over = smt.fit_resample(input_ids[train_df_ind], labels[train_df_ind])\n",
        "  #smt = SMOTE(random_state=0)\n",
        "  #attention_over, _ = smt.fit_resample(attention_masks[train_df_ind], labels[train_df_ind])\n",
        "  train_dataset = TensorDataset(input_ids[train_df_ind], attention_masks[train_df_ind], labels[train_df_ind])\n",
        "  \"\"\"\n",
        "  train_dataset = TensorDataset(torch.LongTensor(inp_over),\n",
        "                                torch.LongTensor(attention_over),\n",
        "                                torch.LongTensor(label_over))\n",
        "  \"\"\"\n",
        "  val_dataset = TensorDataset(input_ids[val_df_ind], attention_masks[val_df_ind], labels[val_df_ind])\n",
        "\n",
        "  dict_hyp[outerind] = {}\n",
        "  for epochs in [10]:\n",
        "    for batch_size in [32]:\n",
        "      print('epoch:batchsize',(epochs, batch_size))\n",
        "      # Create the DataLoaders for our training and validation sets.\n",
        "      # We'll take training samples in random order. \n",
        "      train_dataloader = DataLoader(\n",
        "                  train_dataset,  # The training samples.\n",
        "                  sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                  batch_size = batch_size # Trains with this batch size.\n",
        "              )\n",
        "\n",
        "      # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "      validation_dataloader = DataLoader(\n",
        "                  val_dataset, # The validation samples.\n",
        "                  sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                  batch_size = batch_size # Evaluate with this batch size.\n",
        "              )\n",
        "      # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "      # linear classification layer on top. \n",
        "      \n",
        "      model = BertForSequenceClassification.from_pretrained(\n",
        "          \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "          num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                          # You can increase this for multi-class tasks.   \n",
        "          output_attentions = False, # Whether the model returns attentions weights.\n",
        "          output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "      )\n",
        "\n",
        "      # Tell pytorch to run this model on the GPU.\n",
        "      model.cuda()\n",
        "\n",
        "      # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "      # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "      optimizer = AdamW(model.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                      )\n",
        "\n",
        "      # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "      # (Note that this is not the same as the number of training samples).\n",
        "      total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "      # Create the learning rate scheduler.\n",
        "      scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                  num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                  num_training_steps = total_steps)\n",
        "\n",
        "      # Function to calculate the accuracy of our predictions vs labels\n",
        "      def flat_accuracy(preds, labels):\n",
        "          pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "          labels_flat = labels.flatten()\n",
        "          return f1_score(labels_flat, pred_flat, average='macro')\n",
        "\n",
        "      def format_time(elapsed):\n",
        "          '''\n",
        "          Takes a time in seconds and returns a string hh:mm:ss\n",
        "          '''\n",
        "          # Round to the nearest second.\n",
        "          elapsed_rounded = int(round((elapsed)))\n",
        "          \n",
        "          # Format as hh:mm:ss\n",
        "          return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "      device = torch.device(\"cuda\")\n",
        "      # This training code is based on the `run_glue.py` script here:\n",
        "      # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      embeds = []\n",
        "      for epoch_i in range(0, epochs):\n",
        "          \n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "          #print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss = 0\n",
        "\n",
        "          # Put the model into training mode. Don't be mislead--the call to \n",
        "          # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "          # `dropout` and `batchnorm` layers behave differently during training\n",
        "          # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "          model.train()\n",
        "          embed_temp = None\n",
        "          # For each batch of training data...\n",
        "          for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "              # Progress update every 40 batches.\n",
        "              if step % 40 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  \n",
        "                  # Report progress.\n",
        "                  #print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              # Unpack this training batch from our dataloader. \n",
        "              #\n",
        "              # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "              # `to` method.\n",
        "              #\n",
        "              # `batch` contains three pytorch tensors:\n",
        "              #   [0]: input ids \n",
        "              #   [1]: attention masks\n",
        "              #   [2]: labels \n",
        "              b_input_ids = batch[0].to(device)\n",
        "              b_input_mask = batch[1].to(device)\n",
        "              b_labels = batch[2].to(device)\n",
        "\n",
        "              # Always clear any previously calculated gradients before performing a\n",
        "              # backward pass. PyTorch doesn't do this automatically because \n",
        "              # accumulating the gradients is \"convenient while training RNNs\". \n",
        "              # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "              model.zero_grad()        \n",
        "\n",
        "              # Perform a forward pass (evaluate the model on this training batch).\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              # It returns different numbers of parameters depending on what arguments\n",
        "              # arge given and what flags are set. For our useage here, it returns\n",
        "              # the loss (because we provided labels) and the \"logits\"--the model\n",
        "              # outputs prior to activation.\n",
        "              loss, logits, hidden = model(b_input_ids, \n",
        "                                  token_type_ids=None, \n",
        "                                  attention_mask=b_input_mask, \n",
        "                                  labels=b_labels)\n",
        "\n",
        "              if embed_temp == None:\n",
        "                embed_temp = hidden[0][:,0,:]\n",
        "              else:\n",
        "                embed_temp = torch.cat([embed_temp, hidden[0][:,0,:]])\n",
        "              # Accumulate the training loss over all of the batches so that we can\n",
        "              # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "              # single value; the `.item()` function just returns the Python value \n",
        "              # from the tensor.\n",
        "              total_train_loss += loss.item()\n",
        "\n",
        "              # Perform a backward pass to calculate the gradients.\n",
        "              loss.backward()\n",
        "\n",
        "              # Clip the norm of the gradients to 1.0.\n",
        "              # This is to help prevent the \"exploding gradients\" problem.\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "              # Update parameters and take a step using the computed gradient.\n",
        "              # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "              # modified based on their gradients, the learning rate, etc.\n",
        "              optimizer.step()\n",
        "\n",
        "              # Update the learning rate.\n",
        "              scheduler.step()\n",
        "\n",
        "          embeds.append(embed_temp)\n",
        "          # Calculate the average loss over all of the batches.\n",
        "          avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "          # Measure how long this epoch took.\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          #print(\"\")\n",
        "          #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "          #print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "              \n",
        "          # ========================================\n",
        "          #               Validation\n",
        "          # ========================================\n",
        "          # After the completion of each training epoch, measure our performance on\n",
        "          # our validation set.\n",
        "\n",
        "          #print(\"\")\n",
        "          #print(\"Running Validation...\")\n",
        "\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Put the model in evaluation mode--the dropout layers behave differently\n",
        "          # during evaluation.\n",
        "          model.eval()\n",
        "\n",
        "          # Tracking variables \n",
        "          total_eval_accuracy = 0\n",
        "          total_eval_loss = 0\n",
        "          nb_eval_steps = 0\n",
        "\n",
        "          val_true = []\n",
        "          val_pred = []\n",
        "          # Evaluate data for one epoch\n",
        "          for batch in validation_dataloader:\n",
        "              \n",
        "              # Unpack this training batch from our dataloader. \n",
        "              #\n",
        "              # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "              # the `to` method.\n",
        "              #\n",
        "              # `batch` contains three pytorch tensors:\n",
        "              #   [0]: input ids \n",
        "              #   [1]: attention masks\n",
        "              #   [2]: labels \n",
        "              b_input_ids = batch[0].to(device)\n",
        "              b_input_mask = batch[1].to(device)\n",
        "              b_labels = batch[2].to(device)\n",
        "              \n",
        "              # Tell pytorch not to bother with constructing the compute graph during\n",
        "              # the forward pass, since this is only needed for backprop (training).\n",
        "              with torch.no_grad():        \n",
        "\n",
        "                  # Forward pass, calculate logit predictions.\n",
        "                  # token_type_ids is the same as the \"segment ids\", which \n",
        "                  # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                  # The documentation for this `model` function is here: \n",
        "                  # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                  # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                  # values prior to applying an activation function like the softmax.\n",
        "                  (loss, logits, hidden) = model(b_input_ids, \n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        labels=b_labels)\n",
        "                  \n",
        "              # Accumulate the validation loss.\n",
        "              total_eval_loss += loss.item()\n",
        "\n",
        "              # Move logits and labels to CPU\n",
        "              logits = logits.detach().cpu().numpy()\n",
        "              label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "              #Z# append to val set\n",
        "              val_true.extend(label_ids.tolist())\n",
        "              val_pred.extend(logits.argmax(axis=1).tolist())\n",
        "\n",
        "              # Calculate the accuracy for this batch of test sentences, and\n",
        "              # accumulate it over all batches.\n",
        "              total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "              \n",
        "\n",
        "          # Report the final accuracy for this validation run.\n",
        "          avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "          dict_hyp[outerind].update({(epochs, batch_size):f1_score(val_true,val_pred,average='macro')})\n",
        "          print(\"Val F1: {0:.3f}\".format(f1_score(val_true,val_pred,average='macro')))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.\n",
        "          avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "          \n",
        "          #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "          #print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "\n",
        "          # Record all statistics from this epoch.\n",
        "          training_stats.append(\n",
        "              {\n",
        "                  'epoch': epoch_i + 1,\n",
        "                  'Training Loss': avg_train_loss,\n",
        "                  'Valid. Loss': avg_val_loss,\n",
        "                  'Valid. Accur.': avg_val_accuracy,\n",
        "                  'Training Time': training_time,\n",
        "                  'Validation Time': validation_time\n",
        "              }\n",
        "            )\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "      del device\n",
        "  outerind += 1\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:batchsize (10, 32)\n",
            "======== Epoch 1 / 10 ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val F1: 0.835\n",
            "======== Epoch 2 / 10 ========\n",
            "Val F1: 0.834\n",
            "======== Epoch 3 / 10 ========\n",
            "Val F1: 0.827\n",
            "======== Epoch 4 / 10 ========\n",
            "Val F1: 0.828\n",
            "======== Epoch 5 / 10 ========\n",
            "Val F1: 0.824\n",
            "======== Epoch 6 / 10 ========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2EAVNiZV7z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = '/content/drive/My Drive/Scratch/'\n",
        "IDM = 'Senti-140'\n",
        "torch.save(model.state_dict(), save_path+IDM+'_best_model_state.bin'+'50000_epoch10')\n",
        "#model.load_state_dict(torch.load(save_path+IDM+'_best_model_state.bin'+str(fold_no)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sM2U2cXbzKR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69013b5d-a380-49a9-bd96-a68a12fa725c"
      },
      "source": [
        "# Prepare hiddens, give them to tanmay\n",
        "import pickle\n",
        "embeds_np = [embed.detach().cpu().numpy() for embed in embeds]\n",
        "print(len(embeds_np), embeds_np[0].shape)\n",
        "with open(save_path+'Senti-140-embeds-epoch10','wb') as f:\n",
        "  pickle.dump(embeds_np, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 (4024, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "68X4Q3DJ-BqV",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('hyp-hash100-oe-f1-no-over-3class.pickle','wb') as f:\n",
        "    pickle.dump(dict_hyp, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jTYTGqegDJ5w"
      },
      "source": [
        "# Run from here if hyperparameter search is over"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BZ6AgSocDJUZ",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('hyp-hash100-oe-f1-no-over-3class.pickle','rb') as f:\n",
        "  dict_hyp = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUK-nm6JMcWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QTLNRsqH36QE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "21ed6b7c-4cd5-40ba-cb89-85e1c8324d68"
      },
      "source": [
        "# Set the seed value all over the place to make this reproducible.\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from torch.utils.data import TensorDataset\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "          pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "          labels_flat = labels.flatten()\n",
        "          return f1_score(labels_flat, pred_flat, average='macro')\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "##############################################################################3\n",
        "stats['test_inds'] = []\n",
        "stats['test_preds'] = []\n",
        "stats['test_preds_raw'] = []\n",
        "stats['test_true'] = []\n",
        "stats['test_accuracy'] = []\n",
        "kfOuter = StratifiedKFold(5, shuffle=True, random_state=0)\n",
        "tsta = []\n",
        "trna = []\n",
        "trnl = []\n",
        "tstl = []\n",
        "outerind = 0\n",
        "f1i = 0\n",
        "f1 = np.zeros((100, 3))\n",
        "for train_val_ind, test_ind in kfOuter.split(X, y):\n",
        "  #smt = SMOTE(random_state=0)\n",
        "  #input_oversampled, labels_oversampled = smt.fit_resample(input_ids[train_val_ind], labels[train_val_ind])\n",
        "  #smt = SMOTE(random_state=0)\n",
        "  #attention_oversampled, _ = smt.fit_resample(attention_masks[train_val_ind], labels[train_val_ind])\n",
        "  train_val_dataset = TensorDataset(input_ids[train_val_ind], attention_masks[train_val_ind], labels[train_val_ind])\n",
        "  \"\"\"\n",
        "  train_val_dataset = TensorDataset(torch.LongTensor(input_oversampled), \n",
        "                                    torch.LongTensor(attention_oversampled), \n",
        "                                    torch.LongTensor(labels_oversampled))\n",
        "  \"\"\"\n",
        "  test_dataset = TensorDataset(input_ids[test_ind], attention_masks[test_ind], labels[test_ind])\n",
        "  kfInner = StratifiedKFold(5, shuffle=True, random_state=0)\n",
        "  train_ind, val_ind = next(kfInner.split(train_val_ind, labels[train_val_ind]))\n",
        "  train_df_ind = train_val_ind[train_ind]\n",
        "  val_df_ind = train_val_ind[val_ind]\n",
        "  #smt = SMOTE(random_state=0)\n",
        "  #input_oversampled, labels_oversampled = smt.fit_resample(input_ids[train_df_ind], labels[train_df_ind])\n",
        "  #smt = SMOTE(random_state=0)\n",
        "  #attention_oversampled, _ = smt.fit_resample(attention_masks[train_df_ind], labels[train_df_ind])\n",
        "  \"\"\"\n",
        "  train_dataset =  TensorDataset(torch.LongTensor(input_oversampled), \n",
        "                                    torch.LongTensor(attention_oversampled), \n",
        "                                    torch.LongTensor(labels_oversampled))\n",
        "  \"\"\"\n",
        "  train_dataset = TensorDataset(input_ids[train_df_ind], attention_masks[train_df_ind], labels[train_df_ind])\n",
        "  \n",
        "  epochs, batch_size = max(dict_hyp[outerind], key=lambda x: dict_hyp[outerind][x])\n",
        "  #epochs, batch_size = 4, 32\n",
        "  print('epoch:batchsize',(epochs, batch_size))\n",
        "  # Create the DataLoaders for our training and validation sets.\n",
        "  # We'll take training samples in random order. \n",
        "  train_dataloader = DataLoader(\n",
        "              train_val_dataset,  # The training samples.\n",
        "              sampler = RandomSampler(train_val_dataset), # Select batches randomly\n",
        "              batch_size = batch_size # Trains with this batch size.\n",
        "          )\n",
        "\n",
        "  # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "  test_dataloader = DataLoader(\n",
        "              test_dataset, # The validation samples.\n",
        "              sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "              batch_size = batch_size # Evaluate with this batch size.\n",
        "          )\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  # Tell pytorch to run this model on the GPU.\n",
        "  model.cuda()\n",
        "\n",
        "  # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "  # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "  # (Note that this is not the same as the number of training samples).\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "  # We'll store a number of quantities such as training and validation loss, \n",
        "  # validation accuracy, and timings.\n",
        "  training_stats = []\n",
        "\n",
        "  # Measure the total training time for the whole run.\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "\n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      #print(\"\")\n",
        "      #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      #print('Training...')\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_train_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      train_true = []\n",
        "      train_pred = []\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              #print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          # It returns different numbers of parameters depending on what arguments\n",
        "          # arge given and what flags are set. For our useage here, it returns\n",
        "          # the loss (because we provided labels) and the \"logits\"--the model\n",
        "          # outputs prior to activation.\n",
        "          loss, logits = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask, \n",
        "                              labels=b_labels)\n",
        "          \n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()  \n",
        "          train_true.extend(label_ids.tolist())\n",
        "          train_pred.extend(logits.argmax(axis=1).tolist())\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "      trnl.append(avg_train_loss)\n",
        "      trna.append(f1_score(train_true, train_pred, average='macro'))\n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "\n",
        "      #print(\"\")\n",
        "      #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      #print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      #print(\"\")\n",
        "      #print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      total_eval_accuracy = 0\n",
        "      total_eval_loss = 0\n",
        "      nb_eval_steps = 0\n",
        "\n",
        "      val_true = []\n",
        "      val_pred = []\n",
        "      val_pred_raw = []\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in test_dataloader:\n",
        "          \n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "          # the `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "          \n",
        "          # Tell pytorch not to bother with constructing the compute graph during\n",
        "          # the forward pass, since this is only needed for backprop (training).\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "              # values prior to applying an activation function like the softmax.\n",
        "              (loss, logits) = model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels)\n",
        "              \n",
        "          # Accumulate the validation loss.\n",
        "          total_eval_loss += loss.item()\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          import tensorflow as tf\n",
        "          #Z# append to val set\n",
        "          val_true.extend(label_ids.tolist())\n",
        "          val_pred.extend(logits.argmax(axis=1).tolist())\n",
        "          val_pred_raw.extend(tf.nn.softmax(logits).numpy().max(axis=1).tolist())\n",
        "\n",
        "          # Calculate the accuracy for this batch of test sentences, and\n",
        "          # accumulate it over all batches.\n",
        "          total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "          \n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "      \n",
        "      print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_val_loss = total_eval_loss / len(test_dataloader)\n",
        "      tstl.append(avg_val_loss)\n",
        "      tsta.append(f1_score(val_true, val_pred, average='macro'))\n",
        "      f1[f1i,:] = f1_score(val_true, val_pred, average=None)\n",
        "      f1i += 1\n",
        "      # Measure how long the validation run took.\n",
        "      validation_time = format_time(time.time() - t0)\n",
        "      \n",
        "      #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "      #print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "\n",
        "      # Record all statistics from this epoch.\n",
        "      training_stats.append(\n",
        "          {\n",
        "              'epoch': epoch_i + 1,\n",
        "              'Training Loss': avg_train_loss,\n",
        "              'Valid. Loss': avg_val_loss,\n",
        "              'Valid. Accur.': avg_val_accuracy,\n",
        "              'Training Time': training_time,\n",
        "              'Validation Time': validation_time\n",
        "          }\n",
        "        )\n",
        "  stats['test_inds'].extend(test_ind)\n",
        "  \n",
        "  stats['test_preds'].extend(val_pred)\n",
        "  \n",
        "  stats['test_true'].extend(val_true)\n",
        "  stats['test_preds_raw'].extend(val_pred_raw)\n",
        "  stats['test_accuracy'].append(f1_score(val_true, val_pred, average='macro'))\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "  del device\n",
        "  outerind += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:batchsize (15, 4)\n",
            "  Accuracy: 0.28\n",
            "  Accuracy: 0.38\n",
            "  Accuracy: 0.47\n",
            "  Accuracy: 0.62\n",
            "  Accuracy: 0.50\n",
            "  Accuracy: 0.66\n",
            "  Accuracy: 0.61\n",
            "  Accuracy: 0.60\n",
            "  Accuracy: 0.58\n",
            "  Accuracy: 0.54\n",
            "  Accuracy: 0.58\n",
            "  Accuracy: 0.58\n",
            "  Accuracy: 0.58\n",
            "  Accuracy: 0.58\n",
            "  Accuracy: 0.58\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:43 (h:mm:ss)\n",
            "epoch:batchsize (8, 8)\n",
            "  Accuracy: 0.34\n",
            "  Accuracy: 0.45\n",
            "  Accuracy: 0.45\n",
            "  Accuracy: 0.49\n",
            "  Accuracy: 0.60\n",
            "  Accuracy: 0.52\n",
            "  Accuracy: 0.54\n",
            "  Accuracy: 0.54\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:12 (h:mm:ss)\n",
            "epoch:batchsize (15, 4)\n",
            "  Accuracy: 0.27\n",
            "  Accuracy: 0.27\n",
            "  Accuracy: 0.38\n",
            "  Accuracy: 0.57\n",
            "  Accuracy: 0.66\n",
            "  Accuracy: 0.60\n",
            "  Accuracy: 0.53\n",
            "  Accuracy: 0.61\n",
            "  Accuracy: 0.54\n",
            "  Accuracy: 0.57\n",
            "  Accuracy: 0.54\n",
            "  Accuracy: 0.57\n",
            "  Accuracy: 0.57\n",
            "  Accuracy: 0.53\n",
            "  Accuracy: 0.53\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:30 (h:mm:ss)\n",
            "epoch:batchsize (9, 4)\n",
            "  Accuracy: 0.35\n",
            "  Accuracy: 0.39\n",
            "  Accuracy: 0.37\n",
            "  Accuracy: 0.46\n",
            "  Accuracy: 0.41\n",
            "  Accuracy: 0.46\n",
            "  Accuracy: 0.50\n",
            "  Accuracy: 0.53\n",
            "  Accuracy: 0.53\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:18 (h:mm:ss)\n",
            "epoch:batchsize (8, 8)\n",
            "  Accuracy: 0.23\n",
            "  Accuracy: 0.23\n",
            "  Accuracy: 0.31\n",
            "  Accuracy: 0.30\n",
            "  Accuracy: 0.47\n",
            "  Accuracy: 0.44\n",
            "  Accuracy: 0.55\n",
            "  Accuracy: 0.47\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:12 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6wbzUCiJx0r4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0738c4ae-5d01-4609-8e11-b6304730dcb7"
      },
      "source": [
        "# Verification of train-val-test splits\n",
        "total = train_df_ind.tolist() + val_df_ind.tolist() + test_ind.tolist()\n",
        "sorted(total) == list(range(len(total)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAJP-eL3McWj",
        "colab_type": "code",
        "colab": {},
        "outputId": "998720f2-fb6f-43a8-c90f-b2af24572720"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\"\"\"\n",
        "fig, ax = plt.subplots(5,1,figsize=(10,15))\n",
        "plt.title('Accuracy')\n",
        "for i in range(5):\n",
        "    ax[i].plot(range(50), tsta[i*50:(i+1)*50], label='test accuracy')\n",
        "    ax[i].plot(range(50), trna[i*50:(i+1)*50], label='train accuracy')\n",
        "    ax[i].legend()\n",
        "plt.show()\n",
        "\n",
        "eps = 4\n",
        "fig, ax = plt.subplots(5,1,figsize=(10,15))\n",
        "plt.title('Loss')\n",
        "for i in range(5):\n",
        "    ax[i].plot(range(eps), tstl[i*eps:(i+1)*eps], label='test loss')\n",
        "    ax[i].plot(range(eps), trnl[i*eps:(i+1)*eps], label='train loss')\n",
        "    ax[i].plot(range(eps), tsta[i*eps:(i+1)*eps], label='test accuracy')\n",
        "    ax[i].legend()\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(5,1,figsize=(10,15))\n",
        "plt.title('Accuracy')\n",
        "old = 0\n",
        "for i in range(5):\n",
        "    new = max(dict_hyp[i], key=lambda x: dict_hyp[i][x])[0]\n",
        "    ax[i].plot(range(new-old), trna[old:new], label='train accuracy', color='k')\n",
        "    ax[i].plot(range(new-old), f1[old:new,0], label='-1')\n",
        "    ax[i].plot(range(new-old), f1[old:new,1], label='0')\n",
        "    ax[i].plot(range(new-old), f1[old:new,2], label='1')\n",
        "    ax[i].legend()\n",
        "    old = new\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfig, ax = plt.subplots(5,1,figsize=(10,15))\\nplt.title('Accuracy')\\nfor i in range(5):\\n    ax[i].plot(range(50), tsta[i*50:(i+1)*50], label='test accuracy')\\n    ax[i].plot(range(50), trna[i*50:(i+1)*50], label='train accuracy')\\n    ax[i].legend()\\nplt.show()\\n\\neps = 4\\nfig, ax = plt.subplots(5,1,figsize=(10,15))\\nplt.title('Loss')\\nfor i in range(5):\\n    ax[i].plot(range(eps), tstl[i*eps:(i+1)*eps], label='test loss')\\n    ax[i].plot(range(eps), trnl[i*eps:(i+1)*eps], label='train loss')\\n    ax[i].plot(range(eps), tsta[i*eps:(i+1)*eps], label='test accuracy')\\n    ax[i].legend()\\nplt.show()\\n\\nfig, ax = plt.subplots(5,1,figsize=(10,15))\\nplt.title('Accuracy')\\nold = 0\\nfor i in range(5):\\n    new = max(dict_hyp[i], key=lambda x: dict_hyp[i][x])[0]\\n    ax[i].plot(range(new-old), trna[old:new], label='train accuracy', color='k')\\n    ax[i].plot(range(new-old), f1[old:new,0], label='-1')\\n    ax[i].plot(range(new-old), f1[old:new,1], label='0')\\n    ax[i].plot(range(new-old), f1[old:new,2], label='1')\\n    ax[i].legend()\\n    old = new\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pl686ph63Xb0"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5cydepaMcWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip -q install matplotlib==3.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yMNxFhLepq_j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "94c3b1bc-c118-49c5-9511-ea42ea460406"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = [[29,17,17],[6,112,27],[16,20,102]][::-1]\n",
        "sns.heatmap(np.flipud(confusion_matrix(stats['test_true'], stats['test_preds'])),\n",
        "            fmt = '.3g',\n",
        "            annot=True,annot_kws={\"size\": 20},cbar=False,\n",
        "            xticklabels=[-1,0,1],\n",
        "            yticklabels=[1,0,-1])\n",
        "plt.xlabel('predicted')\n",
        "plt.ylabel('ground truth')\n",
        "plt.ylim([0,3])\n",
        "plt.show()\n",
        "confusion_matrix(stats['test_true'], stats['test_preds'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[47,  4,  8],\n",
              "       [13,  8,  5],\n",
              "       [11,  2, 17]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sPzZgYKWE2LL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "207dea05-0f58-41cd-9247-b495478b7c75"
      },
      "source": [
        "print(f1_score(stats['test_true'], stats['test_preds'], average='macro'))\n",
        "print(f1_score(stats['test_true'], stats['test_preds'], average=None))\n",
        "print(classification_report(np.array(stats['test_true'])-1, np.array(stats['test_preds'])-1))\n",
        "print(stats['test_accuracy'])\n",
        "print(np.mean(stats['test_accuracy']))\n",
        "#stats\n",
        "import pickle\n",
        "#with open('/content/drive/My Drive/Twitter+AQ project/SentimentAnalysis/5fold-hyp.pickle','wb') as f:\n",
        "#  pickle.dump(stats, f)\n",
        "#with open('/content/sample_data/test.pickle','wb') as f:\n",
        "#  pickle.dump(stats['test_inds'],f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5632478632478632\n",
            "[0.72307692 0.4        0.56666667]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.66      0.80      0.72        59\n",
            "           0       0.57      0.31      0.40        26\n",
            "           1       0.57      0.57      0.57        30\n",
            "\n",
            "    accuracy                           0.63       115\n",
            "   macro avg       0.60      0.56      0.56       115\n",
            "weighted avg       0.62      0.63      0.61       115\n",
            "\n",
            "[0.5994939038417298, 0.524731182795699, 0.6071428571428572, 0.5641025641025642, 0.4551282051282051]\n",
            "0.5501197426022111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tWXYpb-Wu5PC",
        "colab": {}
      },
      "source": [
        "dfold = df.copy()\n",
        "df = df.reindex(stats['test_inds'])\n",
        "df['pred'] = np.array(stats['test_preds'])-1\n",
        "df['pred_raw'] = np.array(stats['test_preds_raw'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dcyJx4u-A1Uj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "946786b1-d0b5-4db6-a656-d54e6a0af672"
      },
      "source": [
        "df[df['ZLabel'] == df['pred']][['Tweet text processed','ZLabel','pred','pred_raw']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet text processed</th>\n",
              "      <th>ZLabel</th>\n",
              "      <th>pred</th>\n",
              "      <th>pred_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>not at all a # bjp bhakt, but gautamgambhir is right when he says he has deployed water sprinklers, vacuum cleaner worth 90 cr to reduce pollution and what has arvindkejriwal.. # oddeven scheme marketing budget is a cruel joke, with no longtime results</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.985436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>awesome, so aqi back to 'very poor', well, # oddeven # oddevenscheme seems to be a huge success # delhiairemergency # delhiairpollution # delhincrpollution # delhipollution pandyaaabhas pashyantii</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.570918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>gautamgambhir sir, do you think odd-even is gimmick ? please explain how ? i request please give logical reasoning when you reply. # delhiairemergency # delhincrpollution # delhi # oddevenscheme # oddeven # carpoolsupporter # carpooling</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.498259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>the state of delhi is that of a climate emergency. the national capital can no longer get away with gimmick like # oddeven &amp; banning construction sites. we need long term sustainable solutions &amp; stop the blame game. it's time to own up &amp; act responsibly: # gautamgambhir (ani)pic.twitter.com/drux6toyqq</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.982400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>arvindkejriwal you did 1 thing good by getting # oddeven started. please get this continued. we need air to breathe and road to travel and walk as well. # delhiairpollution # trafficupdates dtptraffic do something about dhaula kuan round about.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.890536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td># oddeven great efforts, pls keep it up..</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.542126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>considering the # smog condition in delhi, can we just increase the # oddeven for few more days. it certainly makes the impact &amp; helps big time. dear arvindkejriwal aamaadmiparty can u please help for # extendoddeven in delhi. thanks</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.512260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td># oddeven. useless scheme which allows most polluting 2 wheelers while banning cng vehicles.</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.895400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>oddeven seems to be a fund collection activity as other than smooth traffic there is no environment improvement. are we so desperate to show our power? aamaadmiparty arvindkejriwal narendramodi pmoindia # oddeven # delhiairpollution ltgovdelhi incdelhi toiindianewshttps://twitter.com/9vinie/status/1194879969710624768 …</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.668576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td># oddeven faills to curb pollution as evidently, cars are not the main cause. it is only a ploy by arvindkejriwal to make money to fight elections. # nomoreoddeven # soniasena # fraudmaridhas # economyzameenpar # nitaambanionmetboard</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.729045</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>78 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                 Tweet text processed  \\\n",
              "18                                                                       not at all a # bjp bhakt, but gautamgambhir is right when he says he has deployed water sprinklers, vacuum cleaner worth 90 cr to reduce pollution and what has arvindkejriwal.. # oddeven scheme marketing budget is a cruel joke, with no longtime results   \n",
              "23                                                                                                                               awesome, so aqi back to 'very poor', well, # oddeven # oddevenscheme seems to be a huge success # delhiairemergency # delhiairpollution # delhincrpollution # delhipollution pandyaaabhas pashyantii   \n",
              "27                                                                                       gautamgambhir sir, do you think odd-even is gimmick ? please explain how ? i request please give logical reasoning when you reply. # delhiairemergency # delhincrpollution # delhi # oddevenscheme # oddeven # carpoolsupporter # carpooling   \n",
              "30                     the state of delhi is that of a climate emergency. the national capital can no longer get away with gimmick like # oddeven & banning construction sites. we need long term sustainable solutions & stop the blame game. it's time to own up & act responsibly: # gautamgambhir (ani)pic.twitter.com/drux6toyqq   \n",
              "33                                                                               arvindkejriwal you did 1 thing good by getting # oddeven started. please get this continued. we need air to breathe and road to travel and walk as well. # delhiairpollution # trafficupdates dtptraffic do something about dhaula kuan round about.   \n",
              "..                                                                                                                                                                                                                                                                                                                                ...   \n",
              "96                                                                                                                                                                                                                                                                                          # oddeven great efforts, pls keep it up..   \n",
              "99                                                                                          considering the # smog condition in delhi, can we just increase the # oddeven for few more days. it certainly makes the impact & helps big time. dear arvindkejriwal aamaadmiparty can u please help for # extendoddeven in delhi. thanks   \n",
              "104                                                                                                                                                                                                                                      # oddeven. useless scheme which allows most polluting 2 wheelers while banning cng vehicles.   \n",
              "108  oddeven seems to be a fund collection activity as other than smooth traffic there is no environment improvement. are we so desperate to show our power? aamaadmiparty arvindkejriwal narendramodi pmoindia # oddeven # delhiairpollution ltgovdelhi incdelhi toiindianewshttps://twitter.com/9vinie/status/1194879969710624768 …   \n",
              "111                                                                                         # oddeven faills to curb pollution as evidently, cars are not the main cause. it is only a ploy by arvindkejriwal to make money to fight elections. # nomoreoddeven # soniasena # fraudmaridhas # economyzameenpar # nitaambanionmetboard   \n",
              "\n",
              "     ZLabel  pred  pred_raw  \n",
              "18     -1.0    -1  0.985436  \n",
              "23      1.0     1  0.570918  \n",
              "27      0.0     0  0.498259  \n",
              "30     -1.0    -1  0.982400  \n",
              "33      1.0     1  0.890536  \n",
              "..      ...   ...       ...  \n",
              "96      1.0     1  0.542126  \n",
              "99      1.0     1  0.512260  \n",
              "104    -1.0    -1  0.895400  \n",
              "108    -1.0    -1  0.668576  \n",
              "111    -1.0    -1  0.729045  \n",
              "\n",
              "[78 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4KRzPY7McXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for outerind in range(5):\n",
        "#    print(max(dict_hyp[outerind], key=lambda x: dict_hyp[outerind][x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7NqNHpO-3glC"
      },
      "source": [
        "### Visualize wrongly predicted results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AqvblxkzjcZH",
        "colab": {},
        "outputId": "eae7245e-056f-49a2-8452-84b4ba6fc79d"
      },
      "source": [
        "df[df['ZLabel'] != df['pred']][['Tweet text processed','ZLabel','pred','pred_raw']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet text processed</th>\n",
              "      <th>ZLabel</th>\n",
              "      <th>pred</th>\n",
              "      <th>pred_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>is it odd or even day, manakgupta? # oddeven</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.804356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>did # delhincr activate some cheat code? # aqi # oddeven pleasantly surprised though!</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.854099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>we asked our followers whether # oddeven policy is effective and should be extended. out of 350 responses, 43% people think that # oddevenscheme is effective and should be extended while 57% don't find it effective. # delhiairpollution # delhipollutionpic.twitter.com/ptrj2u4goq</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.646709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td># oddeven is half baked, need long term solutions: # supremecourthttps://www.gonewsindia.com/latest-news/news-and-politics/odd-even-is-half-baked-need-long-term-solutions-sc-4315 …</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.745083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td># oddeven not helping arvindkejriwal . # delhiairemergency # delhiairpollution # delhiairquality narendramodi priyankagandhi rahulgandhi prakashjavdekar capt_amarinder mlkhattarpic.twitter.com/vj9g3wkejk</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.875449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>this is clearly stated by cpcb data that pollution level on most # oddeven days have come down from last year. arvindkejriwal msisodia …pic.twitter.com/jycxc4ou0x</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.773404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>absolutely; # oddeven can go a long way in tackling traffic chaos as well as # pollution on some long term basis</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.741348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>those questioning did # oddeven help bring down air # pollution ? after # oddeven average decrease in aqi *delhi &gt; 41 points *ghaziabad &gt; 13.3 *noida &gt; 12.3 most improvement seen in delhi …</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.536983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>but why don't # delhi # aap # mla voluntarily follow # oddeven throughout the year ? what say bjp4delhi kapilmishra_ind tajinderbagga ?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.928594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>not against # oddeven scheme\", says manoj tiwari. …pic.twitter.com/bnc0qv4y37\"</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.653571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>so maharashtra also heading for # oddeven with ss, congress and ncp, all different ideology parties, coming together for satta\". let's see how long this \" love triangle \" lasts. poonam_mahajan bjp4maharashtra pmoindia amitshah jpnadda narendramodi\"</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.884138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>surprisingly the traffic ws under super control, my office colleagues from delhi r reaching office(ggn)before time. public transportation got benefitted not just in delhi bt also in ncr . ps: if you think # oddeven can really make a difference …</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.843280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>delhi’s odd-even is over now, but people’s throats stay choked. we look at how other countries have cleaned their air in the past: # oddeven # airpollution # delhismog # smogpic.twitter.com/zdfcdjwmxd</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.504508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>truth be told. # oddeven scheme is a farce. …</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.547501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>sir, arvindkejriwal # oddeven should be extended in view of deteriorating air quality of delhi city.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.559337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td># oddeven it's nothing but failure with so many exemptions.</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.614454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>delhi’s # oddeven is the only and most stringent rule of its kind to curb pollution. data and experts back this! …</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.771230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td># oddeven returns after three-day break, 552 violators fined …</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.625109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>. khanumarfa odd day : pic 1 even day : pic 2 # priyanka_reddy # oddeven # hyderabadhorror # riphumanity # hangrapistspic.twitter.com/bvsphrw9yh</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.685553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>arvindkejriwal hello sirji... please provide us with clean air... # savedelhi....do something...please stop playing vote bank politics...take strict actions asap...health is supreme! make # oddeven permanent for entire winters...else we will be forced to leave delhi.. # leavedelhi</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.897348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>various actions taken against # pollution in # delhi by aamaadmiparty arvindkejriwal are - # oddeven # aforestation # plantation banning # construction &amp; # industries for time being. # watersprinkeling on # roads and # greenambient # promoting use of # cycle, # dtc &amp; # delhimetro.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.843925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>this is what we get after the huge success of # oddeven # oddevenscheme # delhiairemergency # delhiairpollution # delhincrpollution # delhi # delhipollution # delhipic.twitter.com/h5orwxabgi</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.534165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>though short term measure, i appreciate # oddeven initiated by aamaadmiparty and cm shri arvindkejriwal . so what's your long term solution gautam sir? being people elect, you should have answers than questions</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.851308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>no odd-even extenstion in # delhi: # kejriwal # oddeven …</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.723293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>kejriwal's brain runs on odd-even mode. on odd mode he turns the # oddeven scheme off, on even mode he turns the scheme on.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.813299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>no thanks to # oddeven</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.727628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>what steps?? # oddeven ?? even cpcb has informed court today that cars only contribute 3% to pollution. so by odd even, you can reduce max 1.5 % of pollution. what about remaining 98.5 %?</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.495089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>so it proves. # oddeven worked for delhi …</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.798887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>delhi: air quality index (aqi) data as per central pollution control board (cpcb) this morning — major pollutant pm 2.5 at 83 ('satisfactory' category) at anand vihar and at 56 ('satisfactory' category) in the area around lodhi road. # oddeven # kejriwalphirse. i kejriwalpic.twitter.com/mbrgshqzjn</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.746393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>mumbai needs odd even too # oddeven # traffic # mumbaitraffic # stuckforever</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.617008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>my blog with some facts and calculation based on assumptions why # oddeven is a failure bjp4india bjp4haryana bjp4up bjp4delhi nupursharmabjp manojtiwarimp kapilmishra_ind …</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.427421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>delhi government will not extend # oddeven scheme as weather improves in the national capital: delhi cm arvindkejriwalpic.twitter.com/hesnzywni0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.610445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>after supreme court said that # oddeven no solution, arvindkejriwal announces that odd-even won't be extended in delhi</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.848351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>arvindkejriwal request to make # oddeven permanent in delhi throughout the year # pollution # delhiairemergency # delhincrpollution</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.533776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>it's not succesfull, please plan for other alternatives # oddeven # oddevenscheme # delhismog # delhichokes # delhincrpollution arvindkejriwalpic.twitter.com/rlakl9b9xh</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.484242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>the smog rises as soon as the odd-even is lifted. # delhiairpollution # delhiairemergency # oddeven</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.548350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>what you have done to control pollution?? why women's are exempted from # oddeven!!! why? anything substantial swatijaihind or just sending notices?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.906632</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                           Tweet text processed  \\\n",
              "0                                                                                                                                                                                                                                                                  is it odd or even day, manakgupta? # oddeven   \n",
              "41                                                                                                                                                                                                                        did # delhincr activate some cheat code? # aqi # oddeven pleasantly surprised though!   \n",
              "67                       we asked our followers whether # oddeven policy is effective and should be extended. out of 350 responses, 43% people think that # oddevenscheme is effective and should be extended while 57% don't find it effective. # delhiairpollution # delhipollutionpic.twitter.com/ptrj2u4goq   \n",
              "84                                                                                                                         # oddeven is half baked, need long term solutions: # supremecourthttps://www.gonewsindia.com/latest-news/news-and-politics/odd-even-is-half-baked-need-long-term-solutions-sc-4315 …   \n",
              "107                                                                                                 # oddeven not helping arvindkejriwal . # delhiairemergency # delhiairpollution # delhiairquality narendramodi priyankagandhi rahulgandhi prakashjavdekar capt_amarinder mlkhattarpic.twitter.com/vj9g3wkejk   \n",
              "114                                                                                                                                          this is clearly stated by cpcb data that pollution level on most # oddeven days have come down from last year. arvindkejriwal msisodia …pic.twitter.com/jycxc4ou0x   \n",
              "28                                                                                                                                                                                             absolutely; # oddeven can go a long way in tackling traffic chaos as well as # pollution on some long term basis   \n",
              "40                                                                                                                those questioning did # oddeven help bring down air # pollution ? after # oddeven average decrease in aqi *delhi > 41 points *ghaziabad > 13.3 *noida > 12.3 most improvement seen in delhi …   \n",
              "50                                                                                                                                                                      but why don't # delhi # aap # mla voluntarily follow # oddeven throughout the year ? what say bjp4delhi kapilmishra_ind tajinderbagga ?   \n",
              "57                                                                                                                                                                                                                               not against # oddeven scheme\", says manoj tiwari. …pic.twitter.com/bnc0qv4y37\"   \n",
              "13                                                     so maharashtra also heading for # oddeven with ss, congress and ncp, all different ideology parties, coming together for satta\". let's see how long this \" love triangle \" lasts. poonam_mahajan bjp4maharashtra pmoindia amitshah jpnadda narendramodi\"   \n",
              "14                                                        surprisingly the traffic ws under super control, my office colleagues from delhi r reaching office(ggn)before time. public transportation got benefitted not just in delhi bt also in ncr . ps: if you think # oddeven can really make a difference …   \n",
              "31                                                                                                     delhi’s odd-even is over now, but people’s throats stay choked. we look at how other countries have cleaned their air in the past: # oddeven # airpollution # delhismog # smogpic.twitter.com/zdfcdjwmxd   \n",
              "71                                                                                                                                                                                                                                                                truth be told. # oddeven scheme is a farce. …   \n",
              "87                                                                                                                                                                                                         sir, arvindkejriwal # oddeven should be extended in view of deteriorating air quality of delhi city.   \n",
              "95                                                                                                                                                                                                                                                  # oddeven it's nothing but failure with so many exemptions.   \n",
              "109                                                                                                                                                                                          delhi’s # oddeven is the only and most stringent rule of its kind to curb pollution. data and experts back this! …   \n",
              "113                                                                                                                                                                                                                                              # oddeven returns after three-day break, 552 violators fined …   \n",
              "1                                                                                                                                                              . khanumarfa odd day : pic 1 even day : pic 2 # priyanka_reddy # oddeven # hyderabadhorror # riphumanity # hangrapistspic.twitter.com/bvsphrw9yh   \n",
              "12                    arvindkejriwal hello sirji... please provide us with clean air... # savedelhi....do something...please stop playing vote bank politics...take strict actions asap...health is supreme! make # oddeven permanent for entire winters...else we will be forced to leave delhi.. # leavedelhi   \n",
              "21                    various actions taken against # pollution in # delhi by aamaadmiparty arvindkejriwal are - # oddeven # aforestation # plantation banning # construction & # industries for time being. # watersprinkeling on # roads and # greenambient # promoting use of # cycle, # dtc & # delhimetro.   \n",
              "26                                                                                                               this is what we get after the huge success of # oddeven # oddevenscheme # delhiairemergency # delhiairpollution # delhincrpollution # delhi # delhipollution # delhipic.twitter.com/h5orwxabgi   \n",
              "29                                                                                           though short term measure, i appreciate # oddeven initiated by aamaadmiparty and cm shri arvindkejriwal . so what's your long term solution gautam sir? being people elect, you should have answers than questions   \n",
              "36                                                                                                                                                                                                                                                    no odd-even extenstion in # delhi: # kejriwal # oddeven …   \n",
              "45                                                                                                                                                                                  kejriwal's brain runs on odd-even mode. on odd mode he turns the # oddeven scheme off, on even mode he turns the scheme on.   \n",
              "53                                                                                                                                                                                                                                                                                       no thanks to # oddeven   \n",
              "91                                                                                                                  what steps?? # oddeven ?? even cpcb has informed court today that cars only contribute 3% to pollution. so by odd even, you can reduce max 1.5 % of pollution. what about remaining 98.5 %?   \n",
              "105                                                                                                                                                                                                                                                                  so it proves. # oddeven worked for delhi …   \n",
              "6    delhi: air quality index (aqi) data as per central pollution control board (cpcb) this morning — major pollutant pm 2.5 at 83 ('satisfactory' category) at anand vihar and at 56 ('satisfactory' category) in the area around lodhi road. # oddeven # kejriwalphirse. i kejriwalpic.twitter.com/mbrgshqzjn   \n",
              "22                                                                                                                                                                                                                                 mumbai needs odd even too # oddeven # traffic # mumbaitraffic # stuckforever   \n",
              "35                                                                                                                                my blog with some facts and calculation based on assumptions why # oddeven is a failure bjp4india bjp4haryana bjp4up bjp4delhi nupursharmabjp manojtiwarimp kapilmishra_ind …   \n",
              "51                                                                                                                                                             delhi government will not extend # oddeven scheme as weather improves in the national capital: delhi cm arvindkejriwalpic.twitter.com/hesnzywni0   \n",
              "52                                                                                                                                                                                       after supreme court said that # oddeven no solution, arvindkejriwal announces that odd-even won't be extended in delhi   \n",
              "66                                                                                                                                                                          arvindkejriwal request to make # oddeven permanent in delhi throughout the year # pollution # delhiairemergency # delhincrpollution   \n",
              "74                                                                                                                                     it's not succesfull, please plan for other alternatives # oddeven # oddevenscheme # delhismog # delhichokes # delhincrpollution arvindkejriwalpic.twitter.com/rlakl9b9xh   \n",
              "77                                                                                                                                                                                                          the smog rises as soon as the odd-even is lifted. # delhiairpollution # delhiairemergency # oddeven   \n",
              "79                                                                                                                                                         what you have done to control pollution?? why women's are exempted from # oddeven!!! why? anything substantial swatijaihind or just sending notices?   \n",
              "\n",
              "     ZLabel  pred  pred_raw  \n",
              "0       0.0     1  0.804356  \n",
              "41      1.0    -1  0.854099  \n",
              "67      0.0    -1  0.646709  \n",
              "84     -1.0     0  0.745083  \n",
              "107    -1.0     0  0.875449  \n",
              "114     1.0    -1  0.773404  \n",
              "28      1.0    -1  0.741348  \n",
              "40      1.0    -1  0.536983  \n",
              "50      0.0    -1  0.928594  \n",
              "57      0.0    -1  0.653571  \n",
              "13      0.0    -1  0.884138  \n",
              "14      1.0    -1  0.843280  \n",
              "31      0.0    -1  0.504508  \n",
              "71     -1.0     1  0.547501  \n",
              "87      1.0    -1  0.559337  \n",
              "95     -1.0     1  0.614454  \n",
              "109     1.0    -1  0.771230  \n",
              "113     0.0    -1  0.625109  \n",
              "1       0.0    -1  0.685553  \n",
              "12      1.0    -1  0.897348  \n",
              "21      0.0    -1  0.843925  \n",
              "26      1.0    -1  0.534165  \n",
              "29      1.0    -1  0.851308  \n",
              "36      0.0    -1  0.723293  \n",
              "45      0.0    -1  0.813299  \n",
              "53     -1.0     1  0.727628  \n",
              "91     -1.0     0  0.495089  \n",
              "105     1.0    -1  0.798887  \n",
              "6       1.0    -1  0.746393  \n",
              "22      1.0     0  0.617008  \n",
              "35     -1.0     0  0.427421  \n",
              "51      0.0    -1  0.610445  \n",
              "52      0.0    -1  0.848351  \n",
              "66      1.0     0  0.533776  \n",
              "74     -1.0     1  0.484242  \n",
              "77      1.0     0  0.548350  \n",
              "79      0.0    -1  0.906632  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGRnAJ0cMcXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('100oe-f1-no-over-3class.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_vIvYsIMcXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}