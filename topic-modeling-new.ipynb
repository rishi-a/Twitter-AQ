{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Import Data\n",
    "data = pd.read_csv('air-pollution-disease.csv', error_bad_lines=False, delimiter='    ')\n",
    "#document = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "textData = data['Tweet text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub('â€™', '', text)\n",
    "    text = re.sub('œ', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return \" \".join(str(x) for x in result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "textData = [clean_text_round1(item) for item in textData]\n",
    "textData = [clean_text_round2(item) for item in textData]\n",
    "textData = [preprocess(item) for item in textData]\n",
    "textData = pd.DataFrame(textData)\n",
    "textData.to_pickle(\"clean_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             familiar negat effect pollut know damag eye\n",
       "1       rebuild neighbourhood layout act solar calenda...\n",
       "2       filter protect brain pollut learn filter abil ...\n",
       "3                    shortterm effect pollut bloodpressur\n",
       "4       background cool paper europ general germani pa...\n",
       "                              ...                        \n",
       "2022    octob issu look pollut effect brain think orig...\n",
       "2023    intern preval chemic sensit copreval asthma au...\n",
       "2024    cite report show mortal effect youll know poll...\n",
       "2025    diolch fawr iawn mcgarri ddod trafod llygredd ...\n",
       "2026     boost immun fight effect pollut onion help write\n",
       "Name: 0, Length: 2027, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aasima</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abdab</th>\n",
       "      <th>abeyta</th>\n",
       "      <th>abido</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>ablat</th>\n",
       "      <th>...</th>\n",
       "      <th>î²caroten</th>\n",
       "      <th>ðÿ²ðÿ</th>\n",
       "      <th>ðÿš²bus</th>\n",
       "      <th>ðÿšðÿ</th>\n",
       "      <th>ðÿžmental</th>\n",
       "      <th>ðÿžðÿ</th>\n",
       "      <th>ùƒøªø</th>\n",
       "      <th>ùƒùø</th>\n",
       "      <th>ûøªù</th>\n",
       "      <th>ƒðÿš</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2027 rows × 5183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaar  aaron  aasima  abbott  abdab  abeyta  abido  abil  abl  ablat  \\\n",
       "0        0      0       0       0      0       0      0     0    0      0   \n",
       "1        0      0       0       0      0       0      0     0    0      0   \n",
       "2        0      0       0       0      0       0      0     1    0      0   \n",
       "3        0      0       0       0      0       0      0     0    0      0   \n",
       "4        0      0       0       0      0       0      0     0    0      0   \n",
       "...    ...    ...     ...     ...    ...     ...    ...   ...  ...    ...   \n",
       "2022     0      0       0       0      0       0      0     0    0      0   \n",
       "2023     0      0       0       0      0       0      0     0    0      0   \n",
       "2024     0      0       0       0      0       0      0     0    0      0   \n",
       "2025     0      0       0       0      0       0      0     0    0      0   \n",
       "2026     0      0       0       0      0       0      0     0    0      0   \n",
       "\n",
       "      ...  î²caroten  ðÿ²ðÿ  ðÿš²bus  ðÿšðÿ  ðÿžmental  ðÿžðÿ  ùƒøªø  ùƒùø  \\\n",
       "0     ...          0      0        0      0          0      0      0     0   \n",
       "1     ...          0      0        0      0          0      0      0     0   \n",
       "2     ...          0      0        0      0          0      0      0     0   \n",
       "3     ...          0      0        0      0          0      0      0     0   \n",
       "4     ...          0      0        0      0          0      0      0     0   \n",
       "...   ...        ...    ...      ...    ...        ...    ...    ...   ...   \n",
       "2022  ...          0      0        0      0          0      0      0     0   \n",
       "2023  ...          0      0        0      0          0      0      0     0   \n",
       "2024  ...          0      0        0      0          0      0      0     0   \n",
       "2025  ...          0      0        0      0          0      0      0     0   \n",
       "2026  ...          0      0        0      0          0      0      0     0   \n",
       "\n",
       "      ûøªù  ƒðÿš  \n",
       "0        0     0  \n",
       "1        0     0  \n",
       "2        0     0  \n",
       "3        0     0  \n",
       "4        0     0  \n",
       "...    ...   ...  \n",
       "2022     0     0  \n",
       "2023     0     0  \n",
       "2024     0     0  \n",
       "2025     0     0  \n",
       "2026     0     0  \n",
       "\n",
       "[2027 rows x 5183 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(textData[0])\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "#data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_dtm.to_pickle(\"data_dtm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "      <th>2024</th>\n",
       "      <th>2025</th>\n",
       "      <th>2026</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>aaar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aaron</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aasima</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>abbott</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>abdab</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2027 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9     ...  2017  \\\n",
       "aaar       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "aaron      0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "aasima     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "abbott     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "abdab      0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "        2018  2019  2020  2021  2022  2023  2024  2025  2026  \n",
       "aaar       0     0     0     0     0     0     0     0     0  \n",
       "aaron      0     0     0     0     0     0     0     0     0  \n",
       "aasima     0     0     0     0     0     0     0     0     0  \n",
       "abbott     0     0     0     0     0     0     0     0     0  \n",
       "abdab      0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 2027 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data_dtm.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.053*\"effect\" + 0.044*\"pollut\" + 0.017*\"airpollut\" + 0.013*\"health\" + 0.012*\"caus\" + 0.010*\"studi\" + 0.009*\"know\" + 0.008*\"level\" + 0.008*\"harm\" + 0.008*\"death\"'),\n",
       " (1,\n",
       "  '0.066*\"effect\" + 0.060*\"pollut\" + 0.033*\"health\" + 0.016*\"airpollut\" + 0.015*\"peopl\" + 0.013*\"harm\" + 0.012*\"function\" + 0.010*\"public\" + 0.010*\"brain\" + 0.009*\"cognit\"')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.059*\"pollut\" + 0.058*\"effect\" + 0.017*\"peopl\" + 0.017*\"airpollut\" + 0.015*\"function\" + 0.013*\"harm\" + 0.012*\"cognit\" + 0.011*\"studi\" + 0.010*\"damag\" + 0.010*\"long\"'),\n",
       " (1,\n",
       "  '0.061*\"effect\" + 0.050*\"pollut\" + 0.035*\"health\" + 0.016*\"airpollut\" + 0.009*\"public\" + 0.009*\"caus\" + 0.009*\"harm\" + 0.008*\"need\" + 0.007*\"issu\" + 0.007*\"climat\"')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=100)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rishiraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rishiraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>familiar negat effect pollut know damag eye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>rebuild neighbourhood layout act solar calenda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>filter protect brain pollut learn filter abil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>shortterm effect pollut bloodpressur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>background cool paper europ general germani pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022</td>\n",
       "      <td>octob issu look pollut effect brain think orig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023</td>\n",
       "      <td>intern preval chemic sensit copreval asthma au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>cite report show mortal effect youll know poll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>diolch fawr iawn mcgarri ddod trafod llygredd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2026</td>\n",
       "      <td>boost immun fight effect pollut onion help write</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2027 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0           familiar negat effect pollut know damag eye\n",
       "1     rebuild neighbourhood layout act solar calenda...\n",
       "2     filter protect brain pollut learn filter abil ...\n",
       "3                  shortterm effect pollut bloodpressur\n",
       "4     background cool paper europ general germani pa...\n",
       "...                                                 ...\n",
       "2022  octob issu look pollut effect brain think orig...\n",
       "2023  intern preval chemic sensit copreval asthma au...\n",
       "2024  cite report show mortal effect youll know poll...\n",
       "2025  diolch fawr iawn mcgarri ddod trafod llygredd ...\n",
       "2026   boost immun fight effect pollut onion help write\n",
       "\n",
       "[2027 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('clean_data.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>effect pollut damag eye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>neighbourhood act calendar effect equinox larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>filter protect brain pollut filter abil preser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>effect pollut bloodpressur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>background paper europ germani surpris pollut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022</td>\n",
       "      <td>look effect brain parkinson diseas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023</td>\n",
       "      <td>sensit copreval autism effect fragranc consum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>report show effect youll pollut death death ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>diolch fawr iawn mcgarri ddod trafod llygredd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2026</td>\n",
       "      <td>boost immun fight effect pollut onion help</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2027 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0                               effect pollut damag eye\n",
       "1     neighbourhood act calendar effect equinox larg...\n",
       "2     filter protect brain pollut filter abil preser...\n",
       "3                            effect pollut bloodpressur\n",
       "4     background paper europ germani surpris pollut ...\n",
       "...                                                 ...\n",
       "2022                 look effect brain parkinson diseas\n",
       "2023  sensit copreval autism effect fragranc consum ...\n",
       "2024  report show effect youll pollut death death ce...\n",
       "2025  diolch fawr iawn mcgarri ddod trafod llygredd ...\n",
       "2026         boost immun fight effect pollut onion help\n",
       "\n",
       "[2027 rows x 1 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean[0].apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aasima</th>\n",
       "      <th>abdab</th>\n",
       "      <th>abido</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abnorm</th>\n",
       "      <th>abort</th>\n",
       "      <th>abound</th>\n",
       "      <th>...</th>\n",
       "      <th>ªuae</th>\n",
       "      <th>ê³µì</th>\n",
       "      <th>ðÿ²ðÿ</th>\n",
       "      <th>ðÿš²bus</th>\n",
       "      <th>ðÿšðÿ</th>\n",
       "      <th>ðÿžmental</th>\n",
       "      <th>ðÿžðÿ</th>\n",
       "      <th>ùƒùø</th>\n",
       "      <th>ûøªù</th>\n",
       "      <th>ƒðÿš</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2027 rows × 4021 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaar  aaron  aasima  abdab  abido  abil  abl  abnorm  abort  abound  \\\n",
       "0        0      0       0      0      0     0    0       0      0       0   \n",
       "1        0      0       0      0      0     0    0       0      0       0   \n",
       "2        0      0       0      0      0     1    0       0      0       0   \n",
       "3        0      0       0      0      0     0    0       0      0       0   \n",
       "4        0      0       0      0      0     0    0       0      0       0   \n",
       "...    ...    ...     ...    ...    ...   ...  ...     ...    ...     ...   \n",
       "2022     0      0       0      0      0     0    0       0      0       0   \n",
       "2023     0      0       0      0      0     0    0       0      0       0   \n",
       "2024     0      0       0      0      0     0    0       0      0       0   \n",
       "2025     0      0       0      0      0     0    0       0      0       0   \n",
       "2026     0      0       0      0      0     0    0       0      0       0   \n",
       "\n",
       "      ...  ªuae  ê³µì  ðÿ²ðÿ  ðÿš²bus  ðÿšðÿ  ðÿžmental  ðÿžðÿ  ùƒùø  ûøªù  \\\n",
       "0     ...     0     0      0        0      0          0      0     0     0   \n",
       "1     ...     0     0      0        0      0          0      0     0     0   \n",
       "2     ...     0     0      0        0      0          0      0     0     0   \n",
       "3     ...     0     0      0        0      0          0      0     0     0   \n",
       "4     ...     0     0      0        0      0          0      0     0     0   \n",
       "...   ...   ...   ...    ...      ...    ...        ...    ...   ...   ...   \n",
       "2022  ...     0     0      0        0      0          0      0     0     0   \n",
       "2023  ...     0     0      0        0      0          0      0     0     0   \n",
       "2024  ...     0     0      0        0      0          0      0     0     0   \n",
       "2025  ...     0     0      0        0      0          0      0     0     0   \n",
       "2026  ...     0     0      0        0      0          0      0     0     0   \n",
       "\n",
       "      ƒðÿš  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "...    ...  \n",
       "2022     0  \n",
       "2023     0  \n",
       "2024     0  \n",
       "2025     0  \n",
       "2026     0  \n",
       "\n",
       "[2027 rows x 4021 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "cvn = CountVectorizer()\n",
    "data_cvn = cvn.fit_transform(data_nouns[0])\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.093*\"effect\" + 0.079*\"pollut\" + 0.060*\"health\" + 0.017*\"function\" + 0.016*\"peopl\" + 0.015*\"airpollut\" + 0.014*\"brain\" + 0.013*\"cognit\" + 0.011*\"harm\" + 0.011*\"children\"'),\n",
       " (1,\n",
       "  '0.074*\"effect\" + 0.042*\"pollut\" + 0.021*\"harm\" + 0.015*\"airpollut\" + 0.012*\"heart\" + 0.011*\"death\" + 0.010*\"year\" + 0.010*\"work\" + 0.010*\"cancer\" + 0.010*\"action\"')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.067*\"effect\" + 0.052*\"pollut\" + 0.032*\"health\" + 0.028*\"work\" + 0.023*\"mcgarri\" + 0.019*\"studi\" + 0.016*\"today\" + 0.016*\"help\" + 0.015*\"fuel\" + 0.014*\"thank\"'),\n",
       " (1,\n",
       "  '0.068*\"effect\" + 0.049*\"health\" + 0.034*\"pollut\" + 0.026*\"climat\" + 0.017*\"autism\" + 0.017*\"chang\" + 0.013*\"includ\" + 0.011*\"danger\" + 0.011*\"action\" + 0.011*\"heat\"'),\n",
       " (2,\n",
       "  '0.107*\"effect\" + 0.091*\"pollut\" + 0.032*\"health\" + 0.032*\"function\" + 0.029*\"harm\" + 0.027*\"brain\" + 0.025*\"cognit\" + 0.023*\"peopl\" + 0.023*\"airpollut\" + 0.020*\"children\"'),\n",
       " (3,\n",
       "  '0.083*\"effect\" + 0.061*\"pollut\" + 0.029*\"health\" + 0.023*\"airpollut\" + 0.019*\"harm\" + 0.017*\"condit\" + 0.015*\"heart\" + 0.015*\"death\" + 0.014*\"level\" + 0.014*\"year\"')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>familiar negat effect pollut damag eye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>neighbourhood act solar calendar light effect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>filter protect brain pollut filter abil preser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>shortterm effect pollut bloodpressur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>background cool paper europ general germani pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022</td>\n",
       "      <td>octob issu look pollut effect brain parkinson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023</td>\n",
       "      <td>intern preval chemic sensit copreval asthma au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>cite report show mortal effect youll pollut ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>diolch fawr iawn mcgarri ddod trafod llygredd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2026</td>\n",
       "      <td>boost immun fight effect pollut onion help</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2027 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0                familiar negat effect pollut damag eye\n",
       "1     neighbourhood act solar calendar light effect ...\n",
       "2     filter protect brain pollut filter abil preser...\n",
       "3                  shortterm effect pollut bloodpressur\n",
       "4     background cool paper europ general germani pa...\n",
       "...                                                 ...\n",
       "2022  octob issu look pollut effect brain parkinson ...\n",
       "2023  intern preval chemic sensit copreval asthma au...\n",
       "2024  cite report show mortal effect youll pollut ca...\n",
       "2025  diolch fawr iawn mcgarri ddod trafod llygredd ...\n",
       "2026         boost immun fight effect pollut onion help\n",
       "\n",
       "[2027 rows x 1 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean[0].apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aasima</th>\n",
       "      <th>abdab</th>\n",
       "      <th>abeyta</th>\n",
       "      <th>abido</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>ablat</th>\n",
       "      <th>abnorm</th>\n",
       "      <th>...</th>\n",
       "      <th>î²caroten</th>\n",
       "      <th>ðÿ²ðÿ</th>\n",
       "      <th>ðÿš²bus</th>\n",
       "      <th>ðÿšðÿ</th>\n",
       "      <th>ðÿžmental</th>\n",
       "      <th>ðÿžðÿ</th>\n",
       "      <th>ùƒøªø</th>\n",
       "      <th>ùƒùø</th>\n",
       "      <th>ûøªù</th>\n",
       "      <th>ƒðÿš</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2027 rows × 4846 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaar  aaron  aasima  abdab  abeyta  abido  abil  abl  ablat  abnorm  \\\n",
       "0        0      0       0      0       0      0     0    0      0       0   \n",
       "1        0      0       0      0       0      0     0    0      0       0   \n",
       "2        0      0       0      0       0      0     1    0      0       0   \n",
       "3        0      0       0      0       0      0     0    0      0       0   \n",
       "4        0      0       0      0       0      0     0    0      0       0   \n",
       "...    ...    ...     ...    ...     ...    ...   ...  ...    ...     ...   \n",
       "2022     0      0       0      0       0      0     0    0      0       0   \n",
       "2023     0      0       0      0       0      0     0    0      0       0   \n",
       "2024     0      0       0      0       0      0     0    0      0       0   \n",
       "2025     0      0       0      0       0      0     0    0      0       0   \n",
       "2026     0      0       0      0       0      0     0    0      0       0   \n",
       "\n",
       "      ...  î²caroten  ðÿ²ðÿ  ðÿš²bus  ðÿšðÿ  ðÿžmental  ðÿžðÿ  ùƒøªø  ùƒùø  \\\n",
       "0     ...          0      0        0      0          0      0      0     0   \n",
       "1     ...          0      0        0      0          0      0      0     0   \n",
       "2     ...          0      0        0      0          0      0      0     0   \n",
       "3     ...          0      0        0      0          0      0      0     0   \n",
       "4     ...          0      0        0      0          0      0      0     0   \n",
       "...   ...        ...    ...      ...    ...        ...    ...    ...   ...   \n",
       "2022  ...          0      0        0      0          0      0      0     0   \n",
       "2023  ...          0      0        0      0          0      0      0     0   \n",
       "2024  ...          0      0        0      0          0      0      0     0   \n",
       "2025  ...          0      0        0      0          0      0      0     0   \n",
       "2026  ...          0      0        0      0          0      0      0     0   \n",
       "\n",
       "      ûøªù  ƒðÿš  \n",
       "0        0     0  \n",
       "1        0     0  \n",
       "2        0     0  \n",
       "3        0     0  \n",
       "4        0     0  \n",
       "...    ...   ...  \n",
       "2022     0     0  \n",
       "2023     0     0  \n",
       "2024     0     0  \n",
       "2025     0     0  \n",
       "2026     0     0  \n",
       "\n",
       "[2027 rows x 4846 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj[0])\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "#data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.027*\"pollut\" + 0.024*\"airpollut\" + 0.018*\"chemic\" + 0.017*\"autism\" + 0.014*\"impact\" + 0.014*\"qualiti\" + 0.014*\"peopl\" + 0.012*\"asthma\" + 0.012*\"level\" + 0.012*\"year\"'),\n",
       " (1,\n",
       "  '0.061*\"pollut\" + 0.046*\"health\" + 0.029*\"caus\" + 0.026*\"public\" + 0.019*\"condit\" + 0.014*\"death\" + 0.013*\"mortal\" + 0.012*\"exercis\" + 0.012*\"harm\" + 0.012*\"requir\"'),\n",
       " (2,\n",
       "  '0.058*\"pollut\" + 0.037*\"climat\" + 0.031*\"health\" + 0.021*\"work\" + 0.020*\"studi\" + 0.020*\"medic\" + 0.019*\"issu\" + 0.019*\"wear\" + 0.018*\"chang\" + 0.018*\"mcgarri\"'),\n",
       " (3,\n",
       "  '0.078*\"pollut\" + 0.032*\"health\" + 0.025*\"airpollut\" + 0.015*\"harm\" + 0.014*\"lung\" + 0.014*\"term\" + 0.012*\"advers\" + 0.009*\"heart\" + 0.009*\"increas\" + 0.008*\"help\"'),\n",
       " (4,\n",
       "  '0.067*\"pollut\" + 0.058*\"function\" + 0.046*\"cognit\" + 0.041*\"peopl\" + 0.037*\"damag\" + 0.030*\"brain\" + 0.029*\"airpollut\" + 0.028*\"children\" + 0.025*\"harm\" + 0.022*\"contamin\"')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.055*\"pollut\" + 0.049*\"health\" + 0.024*\"public\" + 0.022*\"climat\" + 0.015*\"peopl\" + 0.015*\"issu\" + 0.013*\"term\" + 0.012*\"long\" + 0.012*\"medic\" + 0.011*\"coal\"'),\n",
       " (1,\n",
       "  '0.081*\"pollut\" + 0.038*\"function\" + 0.038*\"cognit\" + 0.032*\"peopl\" + 0.030*\"damag\" + 0.029*\"studi\" + 0.028*\"harm\" + 0.024*\"year\" + 0.020*\"work\" + 0.019*\"brain\"'),\n",
       " (2,\n",
       "  '0.072*\"pollut\" + 0.034*\"health\" + 0.018*\"children\" + 0.017*\"reduc\" + 0.017*\"exposur\" + 0.017*\"condit\" + 0.017*\"airpollut\" + 0.015*\"advers\" + 0.013*\"harm\" + 0.011*\"awar\"'),\n",
       " (3,\n",
       "  '0.049*\"pollut\" + 0.028*\"health\" + 0.024*\"harm\" + 0.023*\"caus\" + 0.023*\"airpollut\" + 0.018*\"level\" + 0.017*\"death\" + 0.013*\"mortal\" + 0.013*\"exercis\" + 0.010*\"heart\"'),\n",
       " (4,\n",
       "  '0.042*\"pollut\" + 0.028*\"autism\" + 0.019*\"asthma\" + 0.019*\"airqual\" + 0.018*\"airpollut\" + 0.017*\"brain\" + 0.015*\"lack\" + 0.015*\"consum\" + 0.015*\"function\" + 0.014*\"product\"')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.041*\"health\" + 0.040*\"pollut\" + 0.022*\"issu\" + 0.020*\"airpollut\" + 0.018*\"climat\" + 0.014*\"chemic\" + 0.013*\"peopl\" + 0.013*\"autism\" + 0.013*\"diseas\" + 0.012*\"impact\"'),\n",
       " (1,\n",
       "  '0.056*\"pollut\" + 0.026*\"health\" + 0.022*\"reduc\" + 0.018*\"exposur\" + 0.014*\"airpollut\" + 0.010*\"outdoor\" + 0.009*\"person\" + 0.009*\"condit\" + 0.009*\"intervent\" + 0.008*\"cochran\"'),\n",
       " (2,\n",
       "  '0.068*\"pollut\" + 0.026*\"airpollut\" + 0.024*\"health\" + 0.020*\"lung\" + 0.020*\"risk\" + 0.019*\"public\" + 0.019*\"function\" + 0.018*\"brain\" + 0.018*\"caus\" + 0.016*\"coal\"'),\n",
       " (3,\n",
       "  '0.047*\"pollut\" + 0.022*\"peopl\" + 0.020*\"health\" + 0.020*\"cognit\" + 0.019*\"function\" + 0.019*\"harm\" + 0.015*\"children\" + 0.014*\"public\" + 0.014*\"studi\" + 0.014*\"work\"'),\n",
       " (4,\n",
       "  '0.090*\"pollut\" + 0.034*\"health\" + 0.026*\"caus\" + 0.016*\"level\" + 0.015*\"harm\" + 0.015*\"term\" + 0.012*\"exercis\" + 0.011*\"airpollut\" + 0.010*\"advers\" + 0.009*\"evid\"')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=100)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk, spacy, string\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Import Data\n",
    "dataAP = pd.read_csv('ap-nov19.csv', error_bad_lines=False, delimiter='    ')\n",
    "dataTrump = pd.read_csv('trump-nov19.csv', error_bad_lines=False, delimiter='    ')\n",
    "#document = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAP = dataAP['Tweet text']\n",
    "dataTrump = dataTrump['Tweet text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAP = [clean_text_round1(item) for item in dataAP]\n",
    "dataAP = [clean_text_round2(item) for item in dataAP]\n",
    "dataAP = [preprocess(item) for item in dataAP]\n",
    "\n",
    "\n",
    "dataTrump = [clean_text_round1(item) for item in dataTrump]\n",
    "dataTrump = [clean_text_round2(item) for item in dataTrump]\n",
    "dataTrump = [preprocess(item) for item in dataTrump]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "n_features = 4000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "\n",
    "# ignore terms that have a document frequency strictly higher than 95%, \n",
    "# ignore terms that have a document frequency strictly lower than 2\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataTrump)\n",
    "\n",
    "# alpha=0 means no regularization, l1_ratio=.5, the penalty is a combination of L1 and L2\n",
    "nmf = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "nmf_output = nmf.fit_transform(tfidf)\n",
    "\n",
    "def show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=20)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['trump', 'time', 'elect', 'isnt', 'lie', 'need', 'work', 'hate',\n",
       "        'talk', 'countri', 'real', 'year', 'sell', 'youtub', 'help', 'hat',\n",
       "        'corrupt', 'administr', 'reason', 'cult'], dtype='<U15'),\n",
       " array(['realdonaldtrump', 'potus', 'donaldjtrumpjr', 'secpompeo',\n",
       "        'return', 'whitehous', 'believ', 'marcorubio', 'victori', 'care',\n",
       "        'bless', 'good', 'tuckercarlson', 'investig', 'view', 'christma',\n",
       "        'tell', 'lie', 'senategop', 'ingrahamangl'], dtype='<U15'),\n",
       " array(['presid', 'bless', 'lincoln', 'best', 'greatest', 'militari',\n",
       "        'youtub', 'countri', 'better', 'american', 'troop', 'obama',\n",
       "        'ghani', 'happi', 'trump', 'serv', 'hold', 'time', 'year', 'coup'],\n",
       "       dtype='<U15'),\n",
       " array(['know', 'better', 'thing', 'potus', 'didnt', 'need', 'wrong',\n",
       "        'time', 'lincoln', 'sure', 'obama', 'shes', 'lie', 'doesnt',\n",
       "        'brain', 'isnt', 'refus', 'whistleblow', 'wont', 'terrorist'],\n",
       "       dtype='<U15'),\n",
       " array(['love', 'troop', 'militari', 'america', 'crimin', 'desper',\n",
       "        'brilliant', 'candic', 'world', 'boyfriend', 'dayðÿ', 'shit',\n",
       "        'win', 'freedom', 'boss', 'wish', 'toss', 'countri', 'trumpðÿ',\n",
       "        'nbcnew'], dtype='<U15'),\n",
       " array(['like', 'look', 'sound', 'lie', 'doesnt', 'real', 'obama', 'good',\n",
       "        'leav', 'wouldnt', 'second', 'fact', 'zombi', 'idea', 'wish',\n",
       "        'cheat', 'prove', 'today', 'sure', 'pay'], dtype='<U15'),\n",
       " array(['thank', 'troop', 'thanksgiv', 'hong', 'amaz', 'share', 'good',\n",
       "        'year', 'amen', 'sign', 'kong', 'grate', 'happi', 'candic', 'help',\n",
       "        'best', 'import', 'care', 'wonder', 'send'], dtype='<U15'),\n",
       " array(['vote', 'didnt', 'ballot', 'farmer', 'republican', 'trump',\n",
       "        'machin', 'need', 'voter', 'buy', 'white', 'blue', 'russia',\n",
       "        'remov', 'black', 'dem', 'save', 'stupid', 'senat', 'candid'],\n",
       "       dtype='<U15'),\n",
       " array(['think', 'better', 'busi', 'lincoln', 'poll', 'thing', 'maga',\n",
       "        'abraham', 'putin', 'reach', 'friend', 'clear', 'process',\n",
       "        'republican', 'pay', 'number', 'femal', 'potus', 'donat', 'georg'],\n",
       "       dtype='<U15'),\n",
       " array(['say', 'thanksgiv', 'afghanistan', 'fake', 'news', 'surpris',\n",
       "        'troop', 'talk', 'taliban', 'visit', 'media', 'golf', 'founder',\n",
       "        'happi', 'hear', 'democrat', 'lie', 'actual', 'claim', 'watch'],\n",
       "       dtype='<U15'),\n",
       " array(['donald', 'trump', 'thanksgiv', 'real', 'meet', 'youtub', 'black',\n",
       "        'amaz', 'ugli', 'come', 'rent', 'palmerreport', 'racist', 'est',\n",
       "        'headlin', 'refus', 'mock', 'american', 'militari', 'newsweek'],\n",
       "       dtype='<U15'),\n",
       " array(['disgust', 'member', 'absolut', 'exhaust', 'behavior',\n",
       "        'congressman', 'hous', 'cnnpolit', 'lawmak', 'trump', 'senat',\n",
       "        'chanc', 'say', 'mayb', 'crimin', 'white', 'yall', 'polit',\n",
       "        'honour', 'oath'], dtype='<U15'),\n",
       " array(['peopl', 'elect', 'american', 'work', 'black', 'time', 'yeah',\n",
       "        'poll', 'come', 'lie', 'believ', 'person', 'white', 'leav',\n",
       "        'refus', 'popular', 'rent', 'greatest', 'worst', 'million'],\n",
       "       dtype='<U15'),\n",
       " array(['dont', 'care', 'hate', 'year', 'mean', 'agre', 'time', 'american',\n",
       "        'doesnt', 'worri', 'thing', 'speak', 'fact', 'tell', 'point',\n",
       "        'realiz', 'law', 'person', 'kill', 'listen'], dtype='<U15'),\n",
       " array(['impeach', 'hear', 'remov', 'lawyer', 'senat', 'particip',\n",
       "        'inquiri', 'decid', 'deadlin', 'offic', 'women', 'american',\n",
       "        'coup', 'msnbc', 'nadler', 'trump', 'dem', 'choos', 'panel',\n",
       "        'true'], dtype='<U15'),\n",
       " array(['support', 'racist', 'follow', 'troop', 'trump', 'black', 'crimin',\n",
       "        'rest', 'amaz', 'long', 'roll', 'maga', 'yeah', 'best', 'happen',\n",
       "        'wonder', 'deplor', 'tell', 'fine', 'innoc'], dtype='<U15'),\n",
       " array(['great', 'america', 'landslid', 'patriot', 'countri', 'negat',\n",
       "        'eat', 'tweet', 'ladi', 'opportun', 'come', 'scari', 'listen',\n",
       "        'proud', 'read', 'sure', 'gentlemenâ', 'time', 'free', 'leader'],\n",
       "       dtype='<U15'),\n",
       " array(['republican', 'democrat', 'ignor', 'hous', 'pass', 'near',\n",
       "        'better', 'parti', 'lincoln', 'voxdotcom', 'trump', 'white',\n",
       "        'congress', 'believ', 'voter', 'black', 'year', 'beat', 'organ',\n",
       "        'major'], dtype='<U15'),\n",
       " array(['want', 'didnt', 'tell', 'sure', 'freedom', 'believ', 'good',\n",
       "        'media', 'dem', 'offic', 'request', 'countri', 'somebodi',\n",
       "        'school', 'copi', 'public', 'year', 'person', 'lawyer', 'huge'],\n",
       "       dtype='<U15'),\n",
       " array(['right', 'stand', 'human', 'thing', 'includ', 'investig',\n",
       "        'england', 'servic', 'differ', 'govern', 'make', 'need', 'correct',\n",
       "        'knob', 'evid', 'good', 'corrupt', 'toss', 'fli', 'base'],\n",
       "       dtype='<U15')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "n_features = 4000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "\n",
    "# ignore terms that have a document frequency strictly higher than 95%, \n",
    "# ignore terms that have a document frequency strictly lower than 2\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataAP)\n",
    "\n",
    "# alpha=0 means no regularization, l1_ratio=.5, the penalty is a combination of L1 and L2\n",
    "nmf = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "nmf_output = nmf.fit_transform(tfidf)\n",
    "\n",
    "def show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=20)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Pollution Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['wors', 'suggest', 'think', 'impact', 'studi', 'health', 'pollut',\n",
       "        'guardian', 'previous', 'environ', 'cell', 'bodi', 'affect',\n",
       "        'dirti', 'googlenew', 'suggestsâ', 'hava', 'news', 'sadiqkhan',\n",
       "        'review'], dtype='<U15'),\n",
       " array(['pollut', 'nois', 'water', 'control', 'today', 'area', 'high',\n",
       "        'fight', 'toxic', 'countri', 'check', 'monitor', 'univers',\n",
       "        'christchurch', 'level', 'affect', 'damag', 'outdoor', 'plus',\n",
       "        'sourc'], dtype='<U15'),\n",
       " array(['airpollut', 'airqual', 'cleanaireu', 'health', 'climatechang',\n",
       "        'cleanair', 'environ', 'today', 'delhipollut', 'delhirain',\n",
       "        'research', 'clean', 'purpleair', 'delhiairqu', 'read', 'affect',\n",
       "        'awar', 'tehran', 'forum', 'place'], dtype='<U15'),\n",
       " array(['âµgmâ³', 'cegli', 'messapica', 'martina', 'daili', 'alert',\n",
       "        'averag', 'limit', 'campi', 'httpsna', 'costa', 'minzoni',\n",
       "        'torchiarolodon', 'pollut', 'pleas', 'play', 'platform', 'plastic',\n",
       "        'pleasant', 'plantat'], dtype='<U15'),\n",
       " array(['link', 'problem', 'heart', 'previous', 'health', 'shortterm',\n",
       "        'diseas', 'exposur', 'number', 'cancer', 'stroke', 'condit',\n",
       "        'failur', 'studi', 'grow', 'lung', 'list', 'infect', 'research',\n",
       "        'tract'], dtype='<U15'),\n",
       " array(['climatestrik', 'breathelif', 'airpollut', 'humanright', 'auster',\n",
       "        'cleanair', 'kill', 'chennai', 'climatecrisi', 'india', 'immedi',\n",
       "        'leader', 'kid', 'demand', 'action', 'climat', 'plastic',\n",
       "        'pleasant', 'pleas', 'playground'], dtype='<U15'),\n",
       " array(['bori', 'johnson', 'fail', 'trust', 'chang', 'climat', 'dont',\n",
       "        'year', 'greenjennyjon', 'watch', 'report', 'hold', 'negat',\n",
       "        'pollut', 'buri', 'write', 'damn', 'real', 'huffpost', 'mayor'],\n",
       "       dtype='<U15'),\n",
       " array(['korea', 'shut', 'south', 'quarter', 'winter', 'plant', 'coal',\n",
       "        'coalfir', 'generat', 'tackl', 'reduc', 'pollut', 'power', 'month',\n",
       "        'suspend', 'southkorea', 'effort', 'curb', 'donald', 'electr'],\n",
       "       dtype='<U15'),\n",
       " array(['day', 'level', 'visit', 'higher', 'hospit', 'pollut', 'scientist',\n",
       "        'chelsea', 'admiss', 'scienc', 'cdnpoli', 'surpris', 'viral',\n",
       "        'news', 'high', 'lower', 'increas', 'trend', 'time', 'number'],\n",
       "       dtype='<U15'),\n",
       " array(['reduc', 'need', 'citi', 'climat', 'public', 'transport',\n",
       "        'environ', 'help', 'live', 'clean', 'issu', 'pollut', 'chang',\n",
       "        'india', 'like', 'traffic', 'level', 'action', 'cleanaireu',\n",
       "        'qualiti'], dtype='<U15'),\n",
       " array(['sepsi', 'uti', 'spike', 'respons', 'particl', 'pollut', 'kidney',\n",
       "        'woodburn', 'unsaf', 'failur', 'diseas', 'creat', 'admiss',\n",
       "        'airborn', 'local', 'particular', 'onair', 'urban', 'plant',\n",
       "        'plantat'], dtype='<U15'),\n",
       " array(['delhi', 'delhiairqu', 'india', 'help', 'poison', 'arvindkejriw',\n",
       "        'solv', 'kejriw', 'delhipollut', 'improv', 'rain', 'purifi',\n",
       "        'tower', 'answer', 'aviat', 'delhirain', 'live', 'combat', 'futur',\n",
       "        'talk'], dtype='<U15'),\n",
       " array(['airqual', 'track', 'download', 'skymet', 'locat', 'store',\n",
       "        'qualiti', 'play', 'delhiairqu', 'unhealthi', 'moder', 'airpollut',\n",
       "        'sector', 'noida', 'improv', 'categori', 'delhipollut',\n",
       "        'delhirain', 'airqualityindex', 'news'], dtype='<U15'),\n",
       " array(['caus', 'death', 'link', 'studi', 'admiss', 'hospit', 'pollut',\n",
       "        'year', 'prematur', 'cost', 'medicalxpress', 'million', 'nois',\n",
       "        'kidney', 'earli', 'guidelin', 'europ', 'sepsi', 'emiss', 'govern'],\n",
       "       dtype='<U15'),\n",
       " array(['reason', 'send', 'hospit', 'admiss', 'relat', 'unexpect',\n",
       "        'happen', 'link', 'frequent', 'medicalxpress', 'kidney', 'mayb',\n",
       "        'humanhealth', 'increas', 'presid', 'healthyliv', 'extra',\n",
       "        'america', 'safeti', 'alarm'], dtype='<U15'),\n",
       " array(['effect', 'particul', 'fine', 'health', 'matter', 'beat', 'pollut',\n",
       "        'associ', 'scari', 'stuff', 'bacteria', 'solut', 'report', 'extra',\n",
       "        'exposur', 'behaviour', 'human', 'sever', 'gurjeetchhabra',\n",
       "        'mbumomswhoblog'], dtype='<U15'),\n",
       " array(['peopl', 'kill', 'year', 'million', 'die', 'thousand', 'affect',\n",
       "        'pollut', 'death', 'smoke', 'rise', 'allow', 'work', 'prematur',\n",
       "        'emerg', 'estim', 'auster', 'irishnew', 'live', 'week'],\n",
       "       dtype='<U15'),\n",
       " array(['memori', 'women', 'declin', 'older', 'link', 'httpkslcom',\n",
       "        'kslcom', 'loss', 'pollut', 'elder', 'alzheimerslik', 'greater',\n",
       "        'impair', 'toll', 'english', 'brain', 'evid', 'year', 'scientist',\n",
       "        'expos'], dtype='<U15'),\n",
       " array(['risk', 'glaucoma', 'increas', 'higher', 'link', 'canadian',\n",
       "        'high', 'cancer', 'natobserv', 'million', 'blind', 'littl', 'lung',\n",
       "        'memphi', 'greengood', 'lifestyl', 'pose', 'brain', 'satisfactori',\n",
       "        'airqualityindex'], dtype='<U15'),\n",
       " array(['harm', 'american', 'dose', 'small', 'older', 'medicalxpress',\n",
       "        'pollut', 'benefit', 'rise', 'illeg', 'allow', 'reduct', 'health',\n",
       "        'societi', 'place', 'pledg', 'plan', 'planet', 'pleasant',\n",
       "        'pleasur'], dtype='<U15')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Word 15</th>\n",
       "      <th>Word 16</th>\n",
       "      <th>Word 17</th>\n",
       "      <th>Word 18</th>\n",
       "      <th>Word 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Topic 0</td>\n",
       "      <td>wors</td>\n",
       "      <td>suggest</td>\n",
       "      <td>think</td>\n",
       "      <td>impact</td>\n",
       "      <td>studi</td>\n",
       "      <td>health</td>\n",
       "      <td>pollut</td>\n",
       "      <td>guardian</td>\n",
       "      <td>previous</td>\n",
       "      <td>environ</td>\n",
       "      <td>cell</td>\n",
       "      <td>bodi</td>\n",
       "      <td>affect</td>\n",
       "      <td>dirti</td>\n",
       "      <td>googlenew</td>\n",
       "      <td>suggestsâ</td>\n",
       "      <td>hava</td>\n",
       "      <td>news</td>\n",
       "      <td>sadiqkhan</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 1</td>\n",
       "      <td>pollut</td>\n",
       "      <td>nois</td>\n",
       "      <td>water</td>\n",
       "      <td>control</td>\n",
       "      <td>today</td>\n",
       "      <td>area</td>\n",
       "      <td>high</td>\n",
       "      <td>fight</td>\n",
       "      <td>toxic</td>\n",
       "      <td>countri</td>\n",
       "      <td>check</td>\n",
       "      <td>monitor</td>\n",
       "      <td>univers</td>\n",
       "      <td>christchurch</td>\n",
       "      <td>level</td>\n",
       "      <td>affect</td>\n",
       "      <td>damag</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>plus</td>\n",
       "      <td>sourc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 2</td>\n",
       "      <td>airpollut</td>\n",
       "      <td>airqual</td>\n",
       "      <td>cleanaireu</td>\n",
       "      <td>health</td>\n",
       "      <td>climatechang</td>\n",
       "      <td>cleanair</td>\n",
       "      <td>environ</td>\n",
       "      <td>today</td>\n",
       "      <td>delhipollut</td>\n",
       "      <td>delhirain</td>\n",
       "      <td>research</td>\n",
       "      <td>clean</td>\n",
       "      <td>purpleair</td>\n",
       "      <td>delhiairqu</td>\n",
       "      <td>read</td>\n",
       "      <td>affect</td>\n",
       "      <td>awar</td>\n",
       "      <td>tehran</td>\n",
       "      <td>forum</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 3</td>\n",
       "      <td>âµgmâ³</td>\n",
       "      <td>cegli</td>\n",
       "      <td>messapica</td>\n",
       "      <td>martina</td>\n",
       "      <td>daili</td>\n",
       "      <td>alert</td>\n",
       "      <td>averag</td>\n",
       "      <td>limit</td>\n",
       "      <td>campi</td>\n",
       "      <td>httpsna</td>\n",
       "      <td>costa</td>\n",
       "      <td>minzoni</td>\n",
       "      <td>torchiarolodon</td>\n",
       "      <td>pollut</td>\n",
       "      <td>pleas</td>\n",
       "      <td>play</td>\n",
       "      <td>platform</td>\n",
       "      <td>plastic</td>\n",
       "      <td>pleasant</td>\n",
       "      <td>plantat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 4</td>\n",
       "      <td>link</td>\n",
       "      <td>problem</td>\n",
       "      <td>heart</td>\n",
       "      <td>previous</td>\n",
       "      <td>health</td>\n",
       "      <td>shortterm</td>\n",
       "      <td>diseas</td>\n",
       "      <td>exposur</td>\n",
       "      <td>number</td>\n",
       "      <td>cancer</td>\n",
       "      <td>stroke</td>\n",
       "      <td>condit</td>\n",
       "      <td>failur</td>\n",
       "      <td>studi</td>\n",
       "      <td>grow</td>\n",
       "      <td>lung</td>\n",
       "      <td>list</td>\n",
       "      <td>infect</td>\n",
       "      <td>research</td>\n",
       "      <td>tract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 5</td>\n",
       "      <td>climatestrik</td>\n",
       "      <td>breathelif</td>\n",
       "      <td>airpollut</td>\n",
       "      <td>humanright</td>\n",
       "      <td>auster</td>\n",
       "      <td>cleanair</td>\n",
       "      <td>kill</td>\n",
       "      <td>chennai</td>\n",
       "      <td>climatecrisi</td>\n",
       "      <td>india</td>\n",
       "      <td>immedi</td>\n",
       "      <td>leader</td>\n",
       "      <td>kid</td>\n",
       "      <td>demand</td>\n",
       "      <td>action</td>\n",
       "      <td>climat</td>\n",
       "      <td>plastic</td>\n",
       "      <td>pleasant</td>\n",
       "      <td>pleas</td>\n",
       "      <td>playground</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 6</td>\n",
       "      <td>bori</td>\n",
       "      <td>johnson</td>\n",
       "      <td>fail</td>\n",
       "      <td>trust</td>\n",
       "      <td>chang</td>\n",
       "      <td>climat</td>\n",
       "      <td>dont</td>\n",
       "      <td>year</td>\n",
       "      <td>greenjennyjon</td>\n",
       "      <td>watch</td>\n",
       "      <td>report</td>\n",
       "      <td>hold</td>\n",
       "      <td>negat</td>\n",
       "      <td>pollut</td>\n",
       "      <td>buri</td>\n",
       "      <td>write</td>\n",
       "      <td>damn</td>\n",
       "      <td>real</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 7</td>\n",
       "      <td>korea</td>\n",
       "      <td>shut</td>\n",
       "      <td>south</td>\n",
       "      <td>quarter</td>\n",
       "      <td>winter</td>\n",
       "      <td>plant</td>\n",
       "      <td>coal</td>\n",
       "      <td>coalfir</td>\n",
       "      <td>generat</td>\n",
       "      <td>tackl</td>\n",
       "      <td>reduc</td>\n",
       "      <td>pollut</td>\n",
       "      <td>power</td>\n",
       "      <td>month</td>\n",
       "      <td>suspend</td>\n",
       "      <td>southkorea</td>\n",
       "      <td>effort</td>\n",
       "      <td>curb</td>\n",
       "      <td>donald</td>\n",
       "      <td>electr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 8</td>\n",
       "      <td>day</td>\n",
       "      <td>level</td>\n",
       "      <td>visit</td>\n",
       "      <td>higher</td>\n",
       "      <td>hospit</td>\n",
       "      <td>pollut</td>\n",
       "      <td>scientist</td>\n",
       "      <td>chelsea</td>\n",
       "      <td>admiss</td>\n",
       "      <td>scienc</td>\n",
       "      <td>cdnpoli</td>\n",
       "      <td>surpris</td>\n",
       "      <td>viral</td>\n",
       "      <td>news</td>\n",
       "      <td>high</td>\n",
       "      <td>lower</td>\n",
       "      <td>increas</td>\n",
       "      <td>trend</td>\n",
       "      <td>time</td>\n",
       "      <td>number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 9</td>\n",
       "      <td>reduc</td>\n",
       "      <td>need</td>\n",
       "      <td>citi</td>\n",
       "      <td>climat</td>\n",
       "      <td>public</td>\n",
       "      <td>transport</td>\n",
       "      <td>environ</td>\n",
       "      <td>help</td>\n",
       "      <td>live</td>\n",
       "      <td>clean</td>\n",
       "      <td>issu</td>\n",
       "      <td>pollut</td>\n",
       "      <td>chang</td>\n",
       "      <td>india</td>\n",
       "      <td>like</td>\n",
       "      <td>traffic</td>\n",
       "      <td>level</td>\n",
       "      <td>action</td>\n",
       "      <td>cleanaireu</td>\n",
       "      <td>qualiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 10</td>\n",
       "      <td>sepsi</td>\n",
       "      <td>uti</td>\n",
       "      <td>spike</td>\n",
       "      <td>respons</td>\n",
       "      <td>particl</td>\n",
       "      <td>pollut</td>\n",
       "      <td>kidney</td>\n",
       "      <td>woodburn</td>\n",
       "      <td>unsaf</td>\n",
       "      <td>failur</td>\n",
       "      <td>diseas</td>\n",
       "      <td>creat</td>\n",
       "      <td>admiss</td>\n",
       "      <td>airborn</td>\n",
       "      <td>local</td>\n",
       "      <td>particular</td>\n",
       "      <td>onair</td>\n",
       "      <td>urban</td>\n",
       "      <td>plant</td>\n",
       "      <td>plantat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 11</td>\n",
       "      <td>delhi</td>\n",
       "      <td>delhiairqu</td>\n",
       "      <td>india</td>\n",
       "      <td>help</td>\n",
       "      <td>poison</td>\n",
       "      <td>arvindkejriw</td>\n",
       "      <td>solv</td>\n",
       "      <td>kejriw</td>\n",
       "      <td>delhipollut</td>\n",
       "      <td>improv</td>\n",
       "      <td>rain</td>\n",
       "      <td>purifi</td>\n",
       "      <td>tower</td>\n",
       "      <td>answer</td>\n",
       "      <td>aviat</td>\n",
       "      <td>delhirain</td>\n",
       "      <td>live</td>\n",
       "      <td>combat</td>\n",
       "      <td>futur</td>\n",
       "      <td>talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 12</td>\n",
       "      <td>airqual</td>\n",
       "      <td>track</td>\n",
       "      <td>download</td>\n",
       "      <td>skymet</td>\n",
       "      <td>locat</td>\n",
       "      <td>store</td>\n",
       "      <td>qualiti</td>\n",
       "      <td>play</td>\n",
       "      <td>delhiairqu</td>\n",
       "      <td>unhealthi</td>\n",
       "      <td>moder</td>\n",
       "      <td>airpollut</td>\n",
       "      <td>sector</td>\n",
       "      <td>noida</td>\n",
       "      <td>improv</td>\n",
       "      <td>categori</td>\n",
       "      <td>delhipollut</td>\n",
       "      <td>delhirain</td>\n",
       "      <td>airqualityindex</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 13</td>\n",
       "      <td>caus</td>\n",
       "      <td>death</td>\n",
       "      <td>link</td>\n",
       "      <td>studi</td>\n",
       "      <td>admiss</td>\n",
       "      <td>hospit</td>\n",
       "      <td>pollut</td>\n",
       "      <td>year</td>\n",
       "      <td>prematur</td>\n",
       "      <td>cost</td>\n",
       "      <td>medicalxpress</td>\n",
       "      <td>million</td>\n",
       "      <td>nois</td>\n",
       "      <td>kidney</td>\n",
       "      <td>earli</td>\n",
       "      <td>guidelin</td>\n",
       "      <td>europ</td>\n",
       "      <td>sepsi</td>\n",
       "      <td>emiss</td>\n",
       "      <td>govern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 14</td>\n",
       "      <td>reason</td>\n",
       "      <td>send</td>\n",
       "      <td>hospit</td>\n",
       "      <td>admiss</td>\n",
       "      <td>relat</td>\n",
       "      <td>unexpect</td>\n",
       "      <td>happen</td>\n",
       "      <td>link</td>\n",
       "      <td>frequent</td>\n",
       "      <td>medicalxpress</td>\n",
       "      <td>kidney</td>\n",
       "      <td>mayb</td>\n",
       "      <td>humanhealth</td>\n",
       "      <td>increas</td>\n",
       "      <td>presid</td>\n",
       "      <td>healthyliv</td>\n",
       "      <td>extra</td>\n",
       "      <td>america</td>\n",
       "      <td>safeti</td>\n",
       "      <td>alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 15</td>\n",
       "      <td>effect</td>\n",
       "      <td>particul</td>\n",
       "      <td>fine</td>\n",
       "      <td>health</td>\n",
       "      <td>matter</td>\n",
       "      <td>beat</td>\n",
       "      <td>pollut</td>\n",
       "      <td>associ</td>\n",
       "      <td>scari</td>\n",
       "      <td>stuff</td>\n",
       "      <td>bacteria</td>\n",
       "      <td>solut</td>\n",
       "      <td>report</td>\n",
       "      <td>extra</td>\n",
       "      <td>exposur</td>\n",
       "      <td>behaviour</td>\n",
       "      <td>human</td>\n",
       "      <td>sever</td>\n",
       "      <td>gurjeetchhabra</td>\n",
       "      <td>mbumomswhoblog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 16</td>\n",
       "      <td>peopl</td>\n",
       "      <td>kill</td>\n",
       "      <td>year</td>\n",
       "      <td>million</td>\n",
       "      <td>die</td>\n",
       "      <td>thousand</td>\n",
       "      <td>affect</td>\n",
       "      <td>pollut</td>\n",
       "      <td>death</td>\n",
       "      <td>smoke</td>\n",
       "      <td>rise</td>\n",
       "      <td>allow</td>\n",
       "      <td>work</td>\n",
       "      <td>prematur</td>\n",
       "      <td>emerg</td>\n",
       "      <td>estim</td>\n",
       "      <td>auster</td>\n",
       "      <td>irishnew</td>\n",
       "      <td>live</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 17</td>\n",
       "      <td>memori</td>\n",
       "      <td>women</td>\n",
       "      <td>declin</td>\n",
       "      <td>older</td>\n",
       "      <td>link</td>\n",
       "      <td>httpkslcom</td>\n",
       "      <td>kslcom</td>\n",
       "      <td>loss</td>\n",
       "      <td>pollut</td>\n",
       "      <td>elder</td>\n",
       "      <td>alzheimerslik</td>\n",
       "      <td>greater</td>\n",
       "      <td>impair</td>\n",
       "      <td>toll</td>\n",
       "      <td>english</td>\n",
       "      <td>brain</td>\n",
       "      <td>evid</td>\n",
       "      <td>year</td>\n",
       "      <td>scientist</td>\n",
       "      <td>expos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 18</td>\n",
       "      <td>risk</td>\n",
       "      <td>glaucoma</td>\n",
       "      <td>increas</td>\n",
       "      <td>higher</td>\n",
       "      <td>link</td>\n",
       "      <td>canadian</td>\n",
       "      <td>high</td>\n",
       "      <td>cancer</td>\n",
       "      <td>natobserv</td>\n",
       "      <td>million</td>\n",
       "      <td>blind</td>\n",
       "      <td>littl</td>\n",
       "      <td>lung</td>\n",
       "      <td>memphi</td>\n",
       "      <td>greengood</td>\n",
       "      <td>lifestyl</td>\n",
       "      <td>pose</td>\n",
       "      <td>brain</td>\n",
       "      <td>satisfactori</td>\n",
       "      <td>airqualityindex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 19</td>\n",
       "      <td>harm</td>\n",
       "      <td>american</td>\n",
       "      <td>dose</td>\n",
       "      <td>small</td>\n",
       "      <td>older</td>\n",
       "      <td>medicalxpress</td>\n",
       "      <td>pollut</td>\n",
       "      <td>benefit</td>\n",
       "      <td>rise</td>\n",
       "      <td>illeg</td>\n",
       "      <td>allow</td>\n",
       "      <td>reduct</td>\n",
       "      <td>health</td>\n",
       "      <td>societi</td>\n",
       "      <td>place</td>\n",
       "      <td>pledg</td>\n",
       "      <td>plan</td>\n",
       "      <td>planet</td>\n",
       "      <td>pleasant</td>\n",
       "      <td>pleasur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word 0      Word 1      Word 2      Word 3        Word 4  \\\n",
       "Topic 0           wors     suggest       think      impact         studi   \n",
       "Topic 1         pollut        nois       water     control         today   \n",
       "Topic 2      airpollut     airqual  cleanaireu      health  climatechang   \n",
       "Topic 3         âµgmâ³       cegli   messapica     martina         daili   \n",
       "Topic 4           link     problem       heart    previous        health   \n",
       "Topic 5   climatestrik  breathelif   airpollut  humanright        auster   \n",
       "Topic 6           bori     johnson        fail       trust         chang   \n",
       "Topic 7          korea        shut       south     quarter        winter   \n",
       "Topic 8            day       level       visit      higher        hospit   \n",
       "Topic 9          reduc        need        citi      climat        public   \n",
       "Topic 10         sepsi         uti       spike     respons       particl   \n",
       "Topic 11         delhi  delhiairqu       india        help        poison   \n",
       "Topic 12       airqual       track    download      skymet         locat   \n",
       "Topic 13          caus       death        link       studi        admiss   \n",
       "Topic 14        reason        send      hospit      admiss         relat   \n",
       "Topic 15        effect    particul        fine      health        matter   \n",
       "Topic 16         peopl        kill        year     million           die   \n",
       "Topic 17        memori       women      declin       older          link   \n",
       "Topic 18          risk    glaucoma     increas      higher          link   \n",
       "Topic 19          harm    american        dose       small         older   \n",
       "\n",
       "                 Word 5     Word 6    Word 7         Word 8         Word 9  \\\n",
       "Topic 0          health     pollut  guardian       previous        environ   \n",
       "Topic 1            area       high     fight          toxic        countri   \n",
       "Topic 2        cleanair    environ     today    delhipollut      delhirain   \n",
       "Topic 3           alert     averag     limit          campi        httpsna   \n",
       "Topic 4       shortterm     diseas   exposur         number         cancer   \n",
       "Topic 5        cleanair       kill   chennai   climatecrisi          india   \n",
       "Topic 6          climat       dont      year  greenjennyjon          watch   \n",
       "Topic 7           plant       coal   coalfir        generat          tackl   \n",
       "Topic 8          pollut  scientist   chelsea         admiss         scienc   \n",
       "Topic 9       transport    environ      help           live          clean   \n",
       "Topic 10         pollut     kidney  woodburn          unsaf         failur   \n",
       "Topic 11   arvindkejriw       solv    kejriw    delhipollut         improv   \n",
       "Topic 12          store    qualiti      play     delhiairqu      unhealthi   \n",
       "Topic 13         hospit     pollut      year       prematur           cost   \n",
       "Topic 14       unexpect     happen      link       frequent  medicalxpress   \n",
       "Topic 15           beat     pollut    associ          scari          stuff   \n",
       "Topic 16       thousand     affect    pollut          death          smoke   \n",
       "Topic 17     httpkslcom     kslcom      loss         pollut          elder   \n",
       "Topic 18       canadian       high    cancer      natobserv        million   \n",
       "Topic 19  medicalxpress     pollut   benefit           rise          illeg   \n",
       "\n",
       "                Word 10    Word 11         Word 12       Word 13    Word 14  \\\n",
       "Topic 0            cell       bodi          affect         dirti  googlenew   \n",
       "Topic 1           check    monitor         univers  christchurch      level   \n",
       "Topic 2        research      clean       purpleair    delhiairqu       read   \n",
       "Topic 3           costa    minzoni  torchiarolodon        pollut      pleas   \n",
       "Topic 4          stroke     condit          failur         studi       grow   \n",
       "Topic 5          immedi     leader             kid        demand     action   \n",
       "Topic 6          report       hold           negat        pollut       buri   \n",
       "Topic 7           reduc     pollut           power         month    suspend   \n",
       "Topic 8         cdnpoli    surpris           viral          news       high   \n",
       "Topic 9            issu     pollut           chang         india       like   \n",
       "Topic 10         diseas      creat          admiss       airborn      local   \n",
       "Topic 11           rain     purifi           tower        answer      aviat   \n",
       "Topic 12          moder  airpollut          sector         noida     improv   \n",
       "Topic 13  medicalxpress    million            nois        kidney      earli   \n",
       "Topic 14         kidney       mayb     humanhealth       increas     presid   \n",
       "Topic 15       bacteria      solut          report         extra    exposur   \n",
       "Topic 16           rise      allow            work      prematur      emerg   \n",
       "Topic 17  alzheimerslik    greater          impair          toll    english   \n",
       "Topic 18          blind      littl            lung        memphi  greengood   \n",
       "Topic 19          allow     reduct          health       societi      place   \n",
       "\n",
       "             Word 15      Word 16    Word 17          Word 18          Word 19  \n",
       "Topic 0    suggestsâ         hava       news        sadiqkhan           review  \n",
       "Topic 1       affect        damag    outdoor             plus            sourc  \n",
       "Topic 2       affect         awar     tehran            forum            place  \n",
       "Topic 3         play     platform    plastic         pleasant          plantat  \n",
       "Topic 4         lung         list     infect         research            tract  \n",
       "Topic 5       climat      plastic   pleasant            pleas       playground  \n",
       "Topic 6        write         damn       real         huffpost            mayor  \n",
       "Topic 7   southkorea       effort       curb           donald           electr  \n",
       "Topic 8        lower      increas      trend             time           number  \n",
       "Topic 9      traffic        level     action       cleanaireu          qualiti  \n",
       "Topic 10  particular        onair      urban            plant          plantat  \n",
       "Topic 11   delhirain         live     combat            futur             talk  \n",
       "Topic 12    categori  delhipollut  delhirain  airqualityindex             news  \n",
       "Topic 13    guidelin        europ      sepsi            emiss           govern  \n",
       "Topic 14  healthyliv        extra    america           safeti            alarm  \n",
       "Topic 15   behaviour        human      sever   gurjeetchhabra   mbumomswhoblog  \n",
       "Topic 16       estim       auster   irishnew             live             week  \n",
       "Topic 17       brain         evid       year        scientist            expos  \n",
       "Topic 18    lifestyl         pose      brain     satisfactori  airqualityindex  \n",
       "Topic 19       pledg         plan     planet         pleasant          pleasur  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
